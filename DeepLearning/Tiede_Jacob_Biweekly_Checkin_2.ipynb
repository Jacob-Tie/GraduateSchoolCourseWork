{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biweekly Check-in 2: LSTMs and TabNet\n",
    "# Jacob Tiede\n",
    "## Working with Text Data:\n",
    "For this week I wanted to get some experience working with two new kinds of data: sequential data and tabular data. To this end I will start this report by working with text data. I will be working with data found here: https://www.kaggle.com/c/nlp-getting-started/data?select=train.csv, which is a dataset for disaster classification of tweets, but before we get to that let's look at a state of the art neural net used for these kinds of problems: a long-term short-term memory (LSTM) neural network.\n",
    "## LSTMs\n",
    "I'll start by reviewing an article on LSTMs found here: https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21. This article mentions that LSTMs are a slightly modified version of recurrent neural networks (RNNs), so it is worth taking a brief aside to research these as well:\n",
    "### Aside: RNNs\n",
    "For this section I will be using this article as a guide: https://towardsdatascience.com/recurrent-neural-networks-d4642c9bc7ce. The article details that RNNs seek to solve 2 important problems with conventional neural nets and series data. The first being the shape of the input to an RNN need not be a fixed number, and the second being that an RNN can find relationships in a temporal sense (i.e. it can establish a relationship between inputs that it receives at different time steps). The first is solved by only applying a weight matrix $W_x$ (and presumably a non-linearity) to a small part of the input (maybe a single word for text classification) which will then give what is called the \"hidden state\" of that portion of the input. This is then multiplied with another weight matrix $W_y$ (and non-linearity) to give the first output. Since $W_x$ and $W_y$ stay the same for every input this scheme can be used on inputs of arbitrary size. The second problem (of remembering prior inputs in time) is solved by adding a third weight matrix $W_h$ which takes the hidden state for the previous input and adds $W_h*hidden-state-activation$ to the hidden state of the next input. More concretely, for the nth input to this scheme our hidden state $h_n$ would be $g_1(W_hh_{n-1})+g_2(W_xx_n)$ where $x_n$ is the nth input, and $g_1$ and $g_2$ are functions. This means that as we parse through the input we are propagating the prior hidden states through the neural network. It is worth noting that this is describes a unidirectional RNN, but there is such a thing as a bidirectional RNN (which calculates 2 hidden states, one which will propagate backwards and one the propagates forward), but these are not relevant to our discussion of LSTMs at this time.\n",
    "### Back to LSTMs\n",
    "Moving back to the original article (https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21) we can see that LSTMs are attempting to solve several problems with RNNs most of which are centered around long inputs. Typical RNNs have trouble propagating its memory (the hidden state) through long inputs, so for inputs of say a paragraph of text an RNN will lose information from the beginning. These neural nets are also prone to vanishing and exploding gradients which LSTMs will also attempt to solve by implementing gates into the framework. These gates are a means for the neural network to forget irrelevant information and only remember what is truly important to whatever task it is being trained for. Therefore, it should seem fitting that our first gate is called a \"forget gate\". This gate will take the current input concatenated with the prior hidden state (call this concatenated vector $x_i+h_{i-1}$ and push it through a sigmoid function (effectively giving the neural net the ability to choose to \"forget\" certain aspects of either the input or the prior hidden state since these could either be squished to zero or retained close to one) call the output of this forget gate $f_i$. We then move to an input gate where $x_i+h_{i-1}$ is put through both a sigmoid and a tanh function which creates two vectors which are then multiplied point-wise together producing an output that we'll call $I_i$. $f_i$ is then multiplied pointwise with the previous cell's state giving a vector $l_i$ which is then added point-wise to $I_i$ which gives the cell state of our current cell $c_i$. This then takes us to the final gate: the output gate. This gate will first compute two vectors: first it finds $sigmoid(x_i+h_{i-1})$, then it computes $tanh(c_i)$. These two vectors are then multiplied together pointwise which gives the hidden state of this cell.\n",
    "### Implementation\n",
    "For this section I will use this tutorial from the pytorch documentation as a guide: https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html. The first thing to note is that we can create an \"encoding\" that is trainable, where, rather than just one hot encoding each word into a vector, the program learns how to produce a useful encoding for this task (ie the encoding is trainable). The other major note is that since my neural net only needs to provide a single classification for the tweet as a whole (is there or is there not a disaster) we will only use the last output of the LSTM (ran through a sigmoid) to make our prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is a GPU available?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "print(\"Is a GPU available?\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id keyword location                                               text  \\\n",
      "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
      "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
      "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
      "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
      "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
      "\n",
      "   target  \n",
      "0       1  \n",
      "1       1  \n",
      "2       1  \n",
      "3       1  \n",
      "4       1  \n"
     ]
    }
   ],
   "source": [
    "#Let's load the data into python\n",
    "trainData = pd.read_csv(\"./Tweet_train.csv\")\n",
    "#To get an idea of what the data contains we will print the first few rows:\n",
    "print(trainData.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the target:\n",
    "trainTarget = trainData['target'].to_numpy()\n",
    "#We'll only use the text to train (no keyword or id)\n",
    "trainText = trainData['text'].to_numpy()\n",
    "from sklearn.model_selection import train_test_split\n",
    "trainText, testText, trainTarget, testTarget = train_test_split(trainText, trainTarget, test_size=0.1, random_state=4)\n",
    "\n",
    "testTarget = torch.from_numpy(testTarget).cuda()\n",
    "trainTarget = torch.from_numpy(trainTarget).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (word_embeddings): Embedding(34059, 13)\n",
       "  (lstm): LSTM(13, 6)\n",
       "  (hidden2tag): Linear(in_features=6, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We now need to both separate all of our sentences into words and represent them with a vector, we will do this with a class\n",
    "\n",
    "#This function takes a string and separates it into words\n",
    "def sentToWords(text):\n",
    "    words = []\n",
    "    temp = \"\"\n",
    "    for i in text:\n",
    "        if i != \" \":\n",
    "            temp = temp + i\n",
    "        else:\n",
    "            words.append(temp)\n",
    "            temp = \"\"\n",
    "    words.append(temp)\n",
    "    return words\n",
    "\n",
    "#takes a vector of strings and splits them into words, which then become a vocab dictionary with an index mapping\n",
    "def splitSentences(text):\n",
    "    words = []\n",
    "    temp = \"\"\n",
    "    for para in text:\n",
    "        for i in para:\n",
    "            if i != \" \":\n",
    "                temp = temp + i\n",
    "            else:\n",
    "                words.append(temp)\n",
    "                temp = \"\"\n",
    "        words.append(temp)\n",
    "    mapping = {}\n",
    "    k = 0\n",
    "    for word in words:\n",
    "        if word not in mapping.keys():\n",
    "            mapping[word] = k\n",
    "            k+=1\n",
    "    return mapping\n",
    "vocab = splitSentences(trainText)\n",
    "\n",
    "#converts a sentence to a list of indexes based on a vocab dictionary\n",
    "def sentenceToIdx(sent, mapping):\n",
    "    temp = sentToWords(sent)\n",
    "    mappedSentence = []\n",
    "    for i in temp:\n",
    "        if i in mapping.keys():\n",
    "            mappedSentence.append(mapping[i])\n",
    "        else:\n",
    "            mappedSentence.append(len(mapping.keys()))\n",
    "    return torch.tensor(mappedSentence).view(1,-1)\n",
    "\n",
    "#create the LSTM Classifier, see https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        #the dimension of the hidden state of our LSTM\n",
    "        self.hidden_dim = hidden_dim\n",
    "        #create the trainable word embedding\n",
    "        self.word_embeddings = nn.Embedding(vocab_size+1, embedding_dim)\n",
    "        \n",
    "        #we only have a single LSTM layer in this model\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        #from the pytorch tutorial, they were trying to tag words with things like \"noun\" or \"verb\", but I will only use the final output, so a better name might\n",
    "        #be hidden to prediction, but for the sake of following the notation of the tutorial I will keep the naming\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        #embed the word vectors into a smaller dimension space\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        #send this through our LSTM layer\n",
    "        lstm_out, _ = self.lstm(embeds.view(sentence.shape[1], 1, -1))\n",
    "        #tag space is the \"tags\" of each word, but remember that we only actually need to apply this neural net to the last \"word\" in the sentence (the final output of the LSTM)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(sentence.shape[1], -1)[sentence.shape[1]-1,:])\n",
    "        out = torch.sigmoid(tag_space)\n",
    "        return out\n",
    "model = LSTMClassifier(int(np.floor(np.power(len(vocab.keys()), 1/4))), 6, int(len(vocab.keys())), 1)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAT\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\loss.py:529: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10,  6851] loss: 0.562\n",
      "Test set accuracy for this epoch: 0.6876640419947506\n",
      "[20,  6851] loss: 0.409\n",
      "Test set accuracy for this epoch: 0.7217847769028871\n",
      "[30,  6851] loss: 0.290\n",
      "Test set accuracy for this epoch: 0.7270341207349081\n",
      "[40,  6851] loss: 0.199\n",
      "Test set accuracy for this epoch: 0.7322834645669292\n",
      "[50,  6851] loss: 0.131\n",
      "Test set accuracy for this epoch: 0.7362204724409449\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "#train the model similarly to how we have before\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCELoss()\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    temp = torch.randperm(trainText.shape[0])\n",
    "    trainTarget = trainTarget[temp]\n",
    "    trainText = trainText[temp]\n",
    "    \n",
    "    for j in range(int(trainText.shape[0])):\n",
    "        optimizer.zero_grad()\n",
    "        input = sentenceToIdx(trainText[j], vocab).cuda()\n",
    "        output = model(input)\n",
    "        temp = trainTarget[j]\n",
    "        loss = criterion(output.float(), temp.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if j == trainText.shape[0] -1 and epoch % 10 == 9:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, j + 1, running_loss / trainText.shape[0]))\n",
    "            acc = 0\n",
    "            for k in range(int(testText.shape[0])):\n",
    "                input = sentenceToIdx(testText[k], vocab).cuda()\n",
    "                output = model(input)\n",
    "                if output >= .5:\n",
    "                    output = 1\n",
    "                else:\n",
    "                    output = 0\n",
    "                if output == testTarget[k]:\n",
    "                    acc += 1\n",
    "            print(\"Test set accuracy for this epoch: \" + str(acc/testText.shape[0]))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Thoughts\n",
    "LSTMs performed well for this task, but I wonder if there is a way to get more accuracy, for instance there might be misspelled words in the test data which might make classification difficult.\n",
    "## Changing Gears: TabNet\n",
    "I now want to learn how to use neural nets on yet another type of data: tabular data. To do this I will review Google's TabNet paper (https://arxiv.org/pdf/1908.07442.pdf). This paper seeks to create a deep neural network that replicates important aspects from both tree-based methods for tabular data as well as advantages seen in fields like computer vision (like end-to-end deep learning). This is accomplished by leveraging intuition about how a neural net can be used to emulate a tree, as well as this idea of attention in deep learning. To start the authors give a motivating example of how a neural net can simulate a simple decision tree which uses a lot of the ideas that will come into play for the full model, so I think it is worth going into detail for this example (Figure 3 in the paper):\n",
    "### Aside: Using Neural Nets to Simulate Trees:\n",
    "Consider a set of tabular data with two columns labeled $(x_1, x_2)$ a decision tree will find a set of if-then statements that will determine how to classify points (eg: if $x_1 > 1$ and if $x_2 >1$ then classify $0$). These split points can be thought of as hyperplanes in many dimensions (or in our case a straight line on the $x_1$ $x_2$ plane like $x_1 = 1$). To replicate this behavior in a neural net we should take note that any split point (in the 2D example) is either a horizontal or vertical line (this is generalized later). This means that our neural net should output two vectors (in our case) that represent these decision boundaries (which can then be summed and softmaxed to give a prediction). These vectors will represent on which side of each decision boundary we are for each dimension (in this case 2 dimensions).  \n",
    " \n",
    "Consider a single input $(x_{1i},x_{2j})$ and two different neural nets each of which only see one of the features (again see figure 3 in the paper). This feature selection is accomplished using a mask which is a vector of size $D$ (where $D$ is the number of dimensions in your data) which will be elementwise multiplied with each row of your data. So, in our example to extract the first column our mask would contain a vector $(1,0)$ which, after multiplying with our input $(x_{1i},x_{2j})$, will give $(x_{1i},0)$. This means that our model will need two neural nets, one with a mask containing the vector $(1,0)$ before it and the other with a mask containing the vector $(0,1)$ before it. Let's quickly follow this example through one branch of our structure using the notation from the paper:  \n",
    " \n",
    "We start with an input $(x_{1i},x_{2j})$ which will be passed through our first mask producing $(x_{1i},0)$. This is then passed through a neural net whose output size should be the number of classes you are trying to classify. Since two lines (one vertical and one horizontal) can partition a 2D plane into four distinct sections we will make this neural net have 4 output neurons. However, since this neural net needs only to tell us if we are above or below the a line in $x_1$ space we only need to encode 1 input to two outputs (one of which will be non-zero if we are above the line and one which will be non-zero if we are below the line). Therefore, our weights should be a vector of $[C_1, -C_1, 0, 0]$ with biases $[-aC_1, aC_1, -1, -1]$ in order to produce this behavior. So after running our input $x_{1i}$ through this we will get this vector:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "C_1(x_{1i}-a)\\\\\n",
    "C_1(-x_{1i}+a)\\\\\n",
    "-1\\\\\n",
    "-1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Which is then passed through a ReLU function. Now, say that our line (that we are partitioning the $x_1,x_2$ plane with) is at $x_1 = 1$ to see how this vector indeed partitions our space consider a trained net with $a = 1$ and $C_1 > 0$ then, if $x_{1i} = 2$ our final vector would be:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "C_1(2-1)\\\\\n",
    "C_1(-2+1)\\\\\n",
    "-1\\\\\n",
    "-1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "which, after applying the ReLU becomes:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "C_1\\\\\n",
    "0\\\\\n",
    "0\\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "So every input $x_{1i} > 1$ will always produce a vector with only 1 non-zero component, and that component will always be the first element of the vector. This will work similarly with the second neural net except it will only produce vectors with non-zero components in the third and fourth positions. The outputs of these two neural nets are then added together which will give a vector with two non-zero elements, which will tell you exactly where $(x_{1i},x_{2j})$ is relative to our decision boundaries. Now, it is clear to see that this could generalize easily to lines in multiple dimensions (i.e. a diagonal line in the $x_1,x_2$ plane) it would just require more complex weights, biases, and masks. This is fundamentally the behavior that TabNet is trying to emulate, but with added complexity. Now, before we continue it is pertinent to know what attention is in a deep learning context.\n",
    "### Aside: Attention\n",
    "To get a better understanding of attention I will summarize this blog post on the topic in this section: https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f. To properly summarize this article we can consider natural language processing again, as this is where the first attention mechanism was implemented. Consider again our LSTM network which takes a portion of an input (like a word of a sentence) and encodes it into both a hidden state and a cell state. For the purposes of this discussion consider a slightly different way of thinking where the hidden and cell states are the same state (which I will call the cell state). To do this would require a slight change in thinking, as well as a change to the internal gates of the LSTM block (these are called Gated recurrent units or GRUs, but I won't explain that for now), but for our purposes we can just think of each block or cell as something that takes a word in an outputs a single cell state. Now, we will consider translating a sentence with this network. More concretely, we want to take a sequence of words and turn it into another sequence of words with the same meaning in a different language. The way this problem was first solved was by taking the sequence of words and putting it through the first of two sequence models, and then using the output from the last cell in the first sequence model as the input to the second model to reconstruct the sentence in the target language. Using a diagram this looks like:  \n",
    " \n",
    "sentence -> model 1 -> output vector from the last cell of model 1 -> model 2 -> output of each cell in model 2 is a word in the translation, and we go until we get a special \"end translation\" character  \n",
    " \n",
    "This model runs into problems with memory, elements from the beginning of the sentence are lost, because we are only leveraging information from the very last cell of the first model. The way to solve this is to use all of the cell states across our whole first model, but how do we make the second model leverage all of the information while still understanding where it is in its translation? The answer is attention: for different stages of translation we want our second model to pay more attention to certain parts of the input. For instance, at the start of a translation you would pay more attention to the words in the beginning, which is exactly the behavior we want to replicate for this scheme. Using the notation from the article, we will first run the input sentence through our first model (the \"encoder\") which will produce a number of cell states $h_1,h_2,...,h_n$ which will then be fed into a fully connected neural net along with the previous state of the second model (the \"decoder\"). NOTE: if this is the first word to translate we will just feed the last cell state of the encoder in place of the state of the decoder. We do this because this lets the dense neural net know where in the translation we are, because the goal of this network is to output attention values (or how much attention we should pay each of the states $h_1,h_2,...,h_n$). This neural net calculates \"attention\" by giving an output of size n and having a softmax activation to get a list of probabilities $e_1,e_2,...,e_n$ that all represent the relative importance of each of the encoded states $h_1,h_2,...,h_n$. Element-wise multiplication of the $e$ vector and $h$ vector will then give the so-called \"context vector\" $e_1h_1, e_2h_2, ... , e_nh_n$. We then need to also tell the decoder exactly where we are in the translation so we will append a vector representing the last translated word to this context vector (or a special start character if this is the first word in the translation), and pass this concatenated vector to the decoder which will then give us our next word and cell state to be used in the cycle all over again.  \n",
    " \n",
    "To summarize: attention is a way of leveraging more data without getting overwhelmed by irrelevant details, but more relevantly it is a way of identifying what information is actually important for a given computation. But the question still remains: how does TabNet leverage this concept?\n",
    "### Attention in TabNet\n",
    "Moving back to the paper (to reiterate the source: https://arxiv.org/pdf/1908.07442.pdf) we return to the idea of masks, but we note that a mask is very similar to the vector $e_1,e_2,...,e_n$ that was output by the attention neural net in our translation example. This is, in fact, where the attention mechanism lies, everytime we apply a mask we are choosing a subset of parameters that we are considering 'relevant', and this, much like the translation example, will be trained. We can now talk about the two fundamental building blocks of this model: Feature Transformers and Attention Transformer. We will start with the latter:\n",
    "### Attention Transformers\n",
    "The goal of these transformers is to take a representation of the input from a feature transformer and turn it into a mask. Using the notation of the paper: we will use the feature transformer on the initial data, which has dimension $BxD$ where $B$ is the batch size and $D$ is the number of columns in our data, and transform it into a vector of size $BxN_a$. This is now passed to our attention transformer which will turn it into a mask of size $BxD$. With this goal in mind we can define the neural net at this point:  \n",
    " \n",
    "We start with a fully connected layer followed by batch normalization the output of which is multiplied by a \"prior scales\" function. This function is defined in the paper as:\n",
    "$$\n",
    "P[i] = \\Pi_{j=1}^i (\\gamma - M[j])\n",
    "$$\n",
    "Where $\\gamma$ is a 'relaxation' parameter and $M[j]$ is the jth mask in our model. $P[0]$ is initialized to be all ones. This is then put through a function called a sparse max. I won't go into what exactly this is right now beyond saying that it is like softmax, but it tries to set low probabilities to zero without losing the desirable qualities of softmax (like differentiability). A good article on this topic can be found here: https://towardsdatascience.com/what-is-sparsemax-f84c136624e4. Sparsemax is desirable here because we would like to totally exclude columns if we can, so this allows our mask to do that. After applying this activation we will have the mask that we will use for the next layer, and for the sake of efficiency one can update prior scales by multiplying the stored prior scales by $\\gamma - newMask$.\n",
    "### Feature Transformers\n",
    "These are built up from repeats of 3 layers: a fully connected layer, a batch normalization layer, and a gated linear unit non-linearity. In the paper each feature transformer was made of 4 repeats of these 3 layers, with residual connections between each layer, normalized by a coefficient of $\\sqrt{.5}$. The paper also notes that some of the layers in these transformers should be consistent across the whole network, so we must reuse a subset of these layers throughout the model. It is also noted that there is a split operation performed in these transformers. This is simply splitting the output of this transformer into a decision and an input to the attention transformer. The input to the attention transformer has size $N_a$ and output prediction (decision) for this step will have dimension $N_d$. An important implementation note is that all of the batch normalizations in the feature transformers are what are called ghost batch normalizations which I think are worth talking about:\n",
    "### Ghost Batch Normalization\n",
    "For this section I will review the paper that originally presented ghost batch normalization found here: https://arxiv.org/pdf/1705.08741.pdf. To start let's detail a known problem in machine learning: the generalization gap. When using large batch sizes there is a 'degradation of generalization' in the performance of neural networks, in other words training accuracy might remain the same for large batch sizes, but there is a drop in accuracy on the test set (our model is not generalizing). This is not a problem for small batch sizes, but in order to use the full capability of modern GPUs the authors of the TabNet paper recommend using batch sizes as large as possible. Ghost batch normalization is one proposed solution to this problem. The paper on ghost mini batches (https://arxiv.org/pdf/1705.08741.pdf) finds that the distance of the final weights from their initialization position grows logarithmically with the number of gradient descent steps. Intuitively, taking fewer steps in gradient descent should make the model less accurate, but this result shows that this problem can be much larger than one might anticipate. With this hypothesis they show that it is indeed the case that our \"generalization gap\" stems from not doing as many gradient descent steps when using a larger batch size. This is perhaps a slight simplification, so it is worth noting that small mini-batches have another desirable property: stochastic noise. Since the input is smaller a small mini-batch has a larger variance which (according to the paper) has been found to encourage the weights to move out of steep minima and more toward a global minima. To mitigate these problems they propose the method ghost batch normalization.  \n",
    " \n",
    "The goal of ghost batch normalization is to match the statistics of the step size when using a small batch. This will allow us to leverage all of the desirable properties of a small mini-batch for a large mini-batch. The way ghost batch normalization does this is by taking small \"ghost\" batches (using subsets of the mini-batch that it is processing), and does batch normalization on only those portions of the batch. For the training set it keeps track of the running average and standard deviation in the same way that normal batch normalization would to eventually use on the test set. This method should provide stochastic noise similar to that of small mini-batches, and the authors show that using it will close the generalization gap if we allow the model to perform more iterations of gradient descent.\n",
    "### Overall Structure of TabNet\n",
    "We now must consider the overall arrangement of transformers. To start, we receive data into our model, and we must first get our data's statistics under control. One of the advantages that the author's of TabNet outline is that this network is meant to run in so-called \"end-to-end\" deep learning, this means that we are assuming that we have not preprocessed our features in any way (other than making them numeric). This means that our features could be input with vastly different scales, and overall statistics, so the authors first get all of those things under control with a normal batch normalization layer. We then consider the first block of the model. The data is sent to a block with this structure:  \n",
    " \n",
    "Data (size $BxD$) -> Feature Transformer -> Output of Feature Transformer (size $BxN_a+N_d$) -> split (creates two outputs, one of size $BxN_d$ and one of size $BxN_a$) -> one of these outputs is not needed for this step, specifically the $BxN_d$ sized output -> Feed split's $BxN_a$ output into the attention transformer -> Outputs mask of size $BxD$.  \n",
    " \n",
    "From a qualitative perspective here's what's happening: the data is fed into the feature transformer which makes both a \"decision\" output ($BxN_d$) and an encoded representation of the data ($BxN_a$). In the first step we aren't making a decision, so we throw the decision output away. We then need to run the encoded features through the attention transformer. From this encoded representation the attention transformer figures out which features we need to pay the most attention to, and outputs a mask. It is also worth noting that the data we input to this layer is a copy of the data, because we need this data for each step of the model. This is then put into the next step of the model which is the real building block of the network. This step looks like this:  \n",
    " \n",
    "Data (size $BxD$) -> pointwise multiplication with a mask (output of previous attention transformer) -> Feature Transformer -> Output of Feature Transformer (size $BxN_a+N_d$) -> split  \n",
    " \n",
    "Each output of split now has a different path so we'll start by following the $BxN_d$ output:  \n",
    " \n",
    "Split output (size $BxN_d$) -> ReLU -> add this output point-wise with the output of all the block ahead of it in the network -> Fully Connected layer -> softmax  \n",
    " \n",
    "One can draw parallels between this scheme and residual learning in that we are sort of aggregating the outputs of many blocks together by adding them together. We will now follow the path of the other split output:  \n",
    " \n",
    "Split output (size $BxN_a$) -> attention transformer -> Mask  \n",
    " \n",
    "This is the same as it was for out first block. The next important part of the architecture is the ability for this model to give feature attribution.\n",
    "### Feature Attribution in TabNet\n",
    "In order to get the global importance of each feature to the decision of the model we need only to know the masks at each step, and the relative importance of each mask to the overall decision. Since we already have each mask (the output of the attention transformer) we just have to consider a way of gauging the relative importance of that mask to the final decision. To gauge this, the authors define an aggregation function (the box labeled \"Agg\" on figure 4 in the paper) as the sum of our decision matrix after we apply a ReLU non-linearity. More precisely: let the output of size $BxN_d$ after we apply the split operation to the output of a feature transformer be called $V_D$. To get the relative importance of a given mask we take $ReLU(V_D)$ and sum along each row to produce a vector $v_A$ of size $Bx1$. We can then take our mask $M_k$ (as the mask from the kth block of our model) of size $BxD$ and multiply each row by the corresponding row of $v_A$ which will give a matrix of the form:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "m_{1,1}*v_1 &m_{1,2}*v_1  & ... & m_{1,D}*v_1\\\\\n",
    "m_{2,1}*v_2 &m_{2,2}*v_2  & ... & m_{2,D}*v_2 \\\\\n",
    "\\vdots  & ... & \\ddots   & \\vdots \\\\\n",
    " m_{B,1}*v_B& m_{B,2}*v_B & ... &m_{B,D}*v_B\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Where $v_i$ is the ith element of $v_A$ and $m_{i,j}$ is the i,jth element of our mask $M_k$. If we call this new matrix $M_{new, k}$ we are almost done defining this aggregation. If our model has $N$ total decision blocks then we will let $M_{sum}$ be the point-wise sum of all of our $M_{new, k}$, in other words $\\sum_{n=1}^NM_{new,n} = M_{sum}$. Then, to create the often desirable property that each row sums to 1, we will normalize this by dividing each element of each row of $M_{sum}$ by its corresponding row sum. This will give a new mask that corresponds to the relative importance of each feature in the model for each batch. One can then take the mean across the batch dimension to get a global importance of each feature. The final aspect of training one of these neural nets is self-supervised learning.\n",
    "### Self-Supervised Learning\n",
    "Self-supervised learning simply refers to using the data itself to sort of 'pre-train' a neural net, so that it has an idea of how different features of the dataset interact before it has even begun trying to predict something. For instance, if we had a dataset that had two columns, marital status and relationship status, with values like \"married\" or \"not married\" for the first column and \"in a relationship\" or \"not in a relationship\" for the second, then given that someone is married it would have to follow that they were in a relationship, and if someone is not in a relationship it should follow that they are not married. This presents a learnable correlation of TabNet, and it would be advantageous for TabNet to have some idea about what this correlation is before it attempts any predictions.  \n",
    " \n",
    "To accomplish this the authors present a deceptively simple method. We are going to feed the model an incomplete dataset and try to make it predict values that we take out. This should teach the model correlations between columns without needing to generate more data. The way that we will do this is by taking our \"encoded\" representation (the output of TabNet) and feeding it into a decoder. More explicitly using the notation from the \"Overall Structure of TabNet'' section, we will take the end product of the $BxN_d$ path of split, put it through a dense layer with output size $BxD$ and put it through a feature decoder. This feature decoder will just be N blocks of this pattern:  \n",
    " \n",
    "Feature Transformer (output $BxN_a+N_d$) -> Fully Connected Layer (output $BxD$)  \n",
    " \n",
    "Since we are allowed to have N steps in this decoder we just take the ouput of each step and add them together to get the reconstructed features. Now, with all of the theory out of the way we can work on an implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "#### Ghost Batch Normalization\n",
    "Here I will detail each step of the implementation of TabNet. To start, we'll create the ghost batch normalization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is a GPU available?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "print(\"Is a GPU available?\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ghostBN - creates a ghost batch norm layer as defined by https://arxiv.org/pdf/1705.08741.pdf\n",
    "\n",
    "@initializaton: numCol - number of features in the data passed to ghostBN\n",
    "                ghost_size - the size of each of the ghost batches, will choose 64 if no other value given\n",
    "                Training - Tells us whether or not we are training\n",
    "                \n",
    "@methods:       forward(self, x): forward pass of our ghost batch normalization\n",
    "                setTraining(self, x): tells ghost batch norm whether it is training or not, sets self.training\n",
    "\"\"\"\n",
    "class ghostBN(nn.Module):\n",
    "    def __init__(self, numCol, training = True, ghost_size = 64):\n",
    "        super(ghostBN, self).__init__()\n",
    "        self.ghost_size = ghost_size\n",
    "        self.numCol = numCol\n",
    "        self.training = training\n",
    "        \n",
    "        # We note that ghost batch norm is just running a single batch norm layer over multiple portions of the input. This means that we can define ghost batch\n",
    "        # norm in terms of a single batch norm layer that is applied to multiple parts of the input\n",
    "        self.GBN = nn.BatchNorm1d(self.numCol)\n",
    "    def forward(self, x):\n",
    "        #GBN works in chunks during training, but on the whole thing if we are not training\n",
    "        if self.training:\n",
    "            #chunk the input data into a list of tensors that are all views of x, for an explanation of why we do this see the comments below this\n",
    "            #see https://pytorch.org/docs/stable/generated/torch.chunk.html for tensor.chunk's specifics\n",
    "            chunks = x.chunk(int(np.ceil(x.shape[0] / self.ghost_size)), dim = 0)\n",
    "            #initialize a vector of outputs\n",
    "            out = []\n",
    "            #for each chunked view for our input data, append the result of running batch norm on it to out\n",
    "            for i in chunks:\n",
    "                out.append(self.GBN(i))\n",
    "\n",
    "            #our final output is then the concatination of all of our outputs. We originally chunked it by row (effectively reducing the size of the mini batch)\n",
    "            #as ghost minibatch intends to do, so we now have to rejoin them row-wise\n",
    "            finalOut = torch.cat(out, dim=0)\n",
    "        else:\n",
    "            finalOut = self.GBN(x)\n",
    "            \n",
    "        #Below was my first attempt at writing this, and I'm only keeping it in to explain why it breaks Pytorch's back propogation. Notice how we are slowly \n",
    "        #changing x in place as we iterate through the array. Each of these \"slice\" operations have a backward step in pytorch, and each slice backward has\n",
    "        #a gradient function that acts on all of x. By working on a single array sequentially we are creating new backpropogation steps in place that effect the\n",
    "        #whole array, but we only actually modify a part of the array. This confuses Pytorches backpropogation since the first slice backward operation will\n",
    "        #see many in place changes that it doesn't recognize, so we need to split the input into multiple arrays that we can run batch norm on separately so\n",
    "        #Pytorch knows how to backpropogate, hence the \"chunk\" operation you see above. It is worth noting tat this method will actually work in forward prop.\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            # the number of complete steps of size ghost_size we can take as we walk down the columns\n",
    "            complete_steps = int(np.floor(self.batch_size/self.ghost_size))\n",
    "            # need one extra iteration since we could have some remainder rows \n",
    "            for i in range(complete_steps+1):\n",
    "                startInd = int(i*self.ghost_size)\n",
    "                if i == complete_steps:\n",
    "                    endInd = x.shape[0]\n",
    "                else:\n",
    "                    endInd = int((i+1)*self.ghost_size)\n",
    "                x[startInd:endInd, :] = self.GBN(x[startInd:endInd, :])\n",
    "        else:\n",
    "            x = self.GBN(x)\n",
    "        return x\n",
    "        \"\"\"\n",
    "        return finalOut\n",
    "    \n",
    "    def setTraining(self, x):\n",
    "        self.training = x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse Max Implementation\n",
    "Now is a good time to go more into depth about the sparse max function since we will need to implement this. We'll need two key results for this function to implement it in pytorch: the forward pass, and the backward pass (its derivative). We'll use the original paper (https://arxiv.org/pdf/1602.02068.pdf) to figure out what sparse max is doing. \n",
    "##### The Forward Pass\n",
    "Sparse max is a function that maps vectors in $n$ dimensions to probabilities (specifically it maps these vectors to a probability simplex). Sparse max does this by finding the euclidean projection of a vector onto the probability simplex, which, the paper notes, has a high probability of hitting the boundary of the simplex which will make the resulting projection very sparse. This can be done in linear time with a simple algorithm (algorithm 1 in the paper, copied below):  \n",
    "  \n",
    "Input z  \n",
    "Sort z in descending order as $z_{(1)}>=z_{(2)}>=...>=z_{(K)}$  \n",
    "Let $k(z) = max(k \\in [K] | 1 + kz_{(k)} > \\sum_{j<=k}z_{(j)}$  \n",
    "Let $\\tau(z) = \\frac{-1+\\sum_{j<=k(z)}z(j)}{k(z)}$  \n",
    "Return $P$ where $p_i = max(0, z_i-\\tau(z))$  \n",
    "  \n",
    "Our output probability vector will be of dimension $K$, and $[K] = (1,2,...,K)$, so $k(z)$ finds the max value from 1 to K such that 1 + (specific value of k) * kth element of our vector z is > the cumulative sum of all of the previous elements of z. This means that $k(z)$ is some integer between 1 and K, so the definition of $\\tau(z)$ should now make sense (at least enough to implement it). \n",
    "##### The Backward Pass\n",
    "Working from the Jacobian section of the paper we can see the formula for the derivative is (this is the formula we'll need for gradient descent, so it is actually the jacobian multiplied by a vector $v$):\n",
    "$$\n",
    "J(z)*v = s \\odot (v-\\hat{v}), \\hat{v} = \\frac{\\sum_{j \\in S(z)} v_j}{|S(z)|}\n",
    "$$\n",
    "Where $S(z)$ are the non-zero elements of sparse max, $s$ is an indicator variable of size K (the size of the output) with a 1 if the jth element had a non-zero sparse max value in the output when we ran the forward pass, and $\\odot$ is element-wise multiplication. This can actually be computed very quickly since we will have already calculated $S(z)$ (and implicitly $s$) in the forward pass. We can now implement this function, but to do so we will need to implement it as a new autograd function in Pytorch, a good tutorial for this can be found here: https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: from the pytorch documentation, we must define an alias when using this later in the code like: sparse_max = sparseMax.apply\n",
    "class sparseMax(torch.autograd.Function):\n",
    "    #static methods are used when we don't need to access a specific instance of a class, ctx will be context that can be saved for later use\n",
    "    #portions of the forward code were inspired by the python code for the forward pass given here: https://towardsdatascience.com/what-is-sparsemax-f84c136624e4\n",
    "    #z is assumed to have a shape of (mini batch size, # of features)\n",
    "    @staticmethod\n",
    "    def forward(ctx, z):\n",
    "        # Step 1: sort z\n",
    "        z_sorted, _ = torch.sort(z, dim = 1, descending = True)\n",
    "        \n",
    "        # Step 2: find k(z)\n",
    "        # first create [K]\n",
    "        bracketK = torch.arange(start = 1, end = z.shape[1] + 1).view(1, z.shape[1]).cuda()\n",
    "        # we would like to do this without a for loop (to be efficient), so first we will find all of the 1+[K]*z_k values\n",
    "        kz = 1 + bracketK*z_sorted\n",
    "        # now we must find the cumulative sum of z_j\n",
    "        z_sum = torch.cumsum(z_sorted, dim = 1)\n",
    "        # now we find all of the valid k values\n",
    "        k_valid = kz > z_sum\n",
    "        # find the last value for each part of the mini-batch where k_valid is true, this will give us our k(z) \n",
    "        val_rows, val_cols = torch.where(k_valid)\n",
    "        k_z = torch.zeros(z.shape[0],1).cuda()\n",
    "        for i in range(z.shape[0]):\n",
    "            if len(val_cols[val_rows == i]) == 0:\n",
    "                k_z[i,:] = 1\n",
    "            else:\n",
    "                k_z[i,:] = torch.torch.max(val_cols[val_rows == i]) + 1\n",
    "        \n",
    "        # Step 3: compute T(z)\n",
    "        Tz = torch.zeros(z.shape[0],1).cuda()\n",
    "        for i in range(z.shape[0]):\n",
    "            Tz[i, :] = (z_sum[i, int(k_z[i,0] - 1)] - 1)/k_z[i,0]\n",
    "        \n",
    "        # Step 4: find the output\n",
    "        p = torch.zeros(z.shape[0], z.shape[1]).cuda()\n",
    "        for i in range(z.shape[1]):\n",
    "            p[:,i],_ = torch.max(torch.cat((z[:,i].view(z.shape[0],1) - Tz, torch.zeros(z.shape[0],1).cuda()), dim = 1), dim = 1)\n",
    "        ctx.save_for_backward(p > 0)\n",
    "        return p\n",
    "    #grad_output are the gradients with respect to the loss for the layer ahead of this one\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Read the cached tensor from forward propagation\n",
    "        input, = ctx.saved_tensors\n",
    "        # We clone the prior gradients so that we don't accidentily modify them in place\n",
    "        grads = grad_output.clone()\n",
    "        v_hat = torch.sum(input* grads, dim = 1)/torch.sum(input, dim = 1)\n",
    "        v_hat = v_hat.view(input.shape[0], 1)\n",
    "        grads = input*(grads - v_hat)\n",
    "        return grads\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a couple of quick tests to make sure that sparse max is working as intended:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is what sparse max outputs for a random input x: \n",
      "tensor([[0.5724, 0.3843, 0.0433, 0.0000],\n",
      "        [0.0448, 0.2056, 0.2438, 0.5058],\n",
      "        [0.3507, 0.1684, 0.3263, 0.1546]], device='cuda:0')\n",
      "The sum of each row, this should be 1 for all of the elements in the batch: \n",
      "tensor([1.0000, 1.0000, 1.0000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "sparse_max = sparseMax.apply\n",
    "x = torch.rand(3,4).cuda()\n",
    "y = sparse_max(x)\n",
    "print(\"Here is what sparse max outputs for a random input x: \")\n",
    "print(y)\n",
    "print(\"The sum of each row, this should be 1 for all of the elements in the batch: \")\n",
    "print(torch.sum(y, dim = 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that sparse max has pushed some of the probabilities to 0. With this implementation out of the way we can now move on to implementing the attention transformer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "attTransformer -- defines an attention transformer using the scheme presented in https://arxiv.org/pdf/1908.07442.pdf \n",
    "V2\n",
    "\n",
    "@initialization: n_a - the input dimension of the attention transformer\n",
    "                 n_out - the number of dimensions the attention transformer outputs, this should be a mask so it should be the same dimension as the data\n",
    "                 gamma - the relaxation parameter in the prior scales function, initialized to 1\n",
    "                 \n",
    "@methods:        forward(self, x): the forward pass through this transformer, assume x is of the form (batch_size, n_a)\n",
    "                 getPriorScales(self): returns the output prior scales to be used in the next attention transformer\n",
    "                 setS(self, x): If we are in the self supervised step we will tell our attention transformer which features to use in the first mask\n",
    "                 setTraining(self, x): sets the training variable to x, will tell our model to adjust during validation\n",
    "\"\"\"\n",
    "class attTransformer(nn.Module):\n",
    "    def __init__(self, n_a, n_out, gamma = 1):\n",
    "        super(attTransformer, self).__init__()\n",
    "        #Set params\n",
    "        #-----------------------------\n",
    "        self.n_a = n_a\n",
    "        self.n_out = n_out\n",
    "        self.gamma = gamma\n",
    "        #The fully connected layer that will take n_a features and create an output with the same dimensions as our data\n",
    "        self.FC = nn.Linear(n_a, n_out)\n",
    "        #Our ghost batch normalization layer\n",
    "        self.GBN = ghostBN(n_out)\n",
    "        #only set if we are self-supervised training\n",
    "        self.S = None\n",
    "        #Tells us whether or not to use training methods or switch of evaluation methods\n",
    "        self.training = True\n",
    "        #-----------------------------\n",
    "        self.prior_scales = None\n",
    "            \n",
    "    def forward(self, x):\n",
    "        #check if we need to initialize prior_scales\n",
    "        if self.prior_scales == None:\n",
    "            self.prior_scales = torch.ones(x.shape[0], self.n_out)\n",
    "            self.prior_scales = self.prior_scales.cuda()\n",
    "        #if our data is not evenly divisible by our batch size the last prior scales might have a different size in the batch dimension\n",
    "        if self.prior_scales.shape[0] != x.shape[0]:\n",
    "            self.prior_scales = self.prior_scales[0:x.shape[0], :]\n",
    "        #If we are not training we need to tell GBN to behave differently\n",
    "        if self.training == False:\n",
    "            self.GBN.setTraining(False)\n",
    "        #start by feeding our data into the fully connected layer, then use ghost batch-normalization\n",
    "        x = self.GBN(self.FC(x))\n",
    "        #check if we are self-supervised learning\n",
    "        if self.S == None:\n",
    "            #point-wise multiply x by prior scales\n",
    "            x = self.prior_scales*x\n",
    "        else:\n",
    "            #multiplying by 1-S will force the attention transformer to focus on using features we know\n",
    "            x = (1-self.S)*self.prior_scales*x\n",
    "        #Use sparse max as the activation\n",
    "        sparse_max = sparseMax.apply\n",
    "        x = sparse_max(x)\n",
    "        #update prior_scales for the next layer\n",
    "        self.output_prior_scales = (self.gamma - x)*self.prior_scales\n",
    "        return x\n",
    "        \n",
    "    def getPriorScales(self):\n",
    "        return self.output_prior_scales\n",
    "    \n",
    "    def setS(self, x):\n",
    "        self.S = x\n",
    "        \n",
    "    def setTraining(self, x):\n",
    "        self.training = x\n",
    "    \n",
    "    def setPriorScales(self, x):\n",
    "        self.prior_scales = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Transformer\n",
    "The only somewhat strange thing about this implementation is that these feature transformers have a number of shared layers. In the paper's implementation 2 shared decision layers are used in each model, and two decision specific layers. This means that our feature transformer class will need to be able to input shared layers as well as get them for use in other parts of the model. Implementing this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "featTransformer -- defines a feature transformer using the cheme presented in https://arxiv.org/pdf/1908.07442.pdf\n",
    "\n",
    "@initialization: n_a - the number of features that will be passed to the attention transformer\n",
    "                 n_d - the number of features that will comprise a decision (this will hold our predictions for this step)\n",
    "                 num_features - the number of features in the data that we give to this transformer\n",
    "                 shared_layers - the layers that will be shared across feature transformers, for the first feature transformer this will be None\n",
    "                 num_layers_shared - the number of layers shared across feature transformers\n",
    "                 num_layers_independent - the number of layers that are different for each feature transformer\n",
    "                 \n",
    "@methods:        forward(self, x): the forward pass of the model\n",
    "                 getSharedLayers(self): returns the shared layers for this model\n",
    "                 setTraining(self, x): sets the training variable which tells our model whether we are in the training or validation steps\n",
    "\"\"\"\n",
    "class featTransformer(nn.Module):\n",
    "    def __init__(self, n_a, n_d, num_features, shared_layers = None, num_layers_shared = 2, num_layers_independent = 2):\n",
    "        super(featTransformer, self).__init__()\n",
    "        #Set params\n",
    "        #-----------------------------\n",
    "        self.n_a = n_a\n",
    "        self.n_d = n_d\n",
    "        self.num_features = num_features\n",
    "        #Keep track of whether or not we are training the model\n",
    "        self.training = True\n",
    "        #the shared layers will be passed to all of our feature transformers\n",
    "        self.num_layers_shared = num_layers_shared\n",
    "        self.shared_layers = shared_layers\n",
    "        #-----------------------------\n",
    "        \n",
    "        #check if this is the first feature transformer, if it is we must initialize the shared layers\n",
    "        if self.shared_layers != None:\n",
    "            self.shared_layers = shared_layers\n",
    "        else:\n",
    "            self.shared_layers = nn.ModuleList([nn.Linear(self.num_features, 2*(self.n_a + self.n_d))])\n",
    "            self.shared_layers.append(ghostBN(2*(self.n_a+self.n_d)))\n",
    "            for i in range(self.num_layers_shared - 1):\n",
    "                self.shared_layers.append(nn.Linear(self.n_a + self.n_d, 2*(self.n_a + self.n_d)))\n",
    "                self.shared_layers.append(ghostBN(2*(self.n_a+self.n_d)))\n",
    "                \n",
    "        #set independent layers for this model\n",
    "        self.num_layers_independent = num_layers_independent\n",
    "        self.independent_layers = nn.ModuleList([])\n",
    "        for i in range(self.num_layers_independent):\n",
    "            self.independent_layers.append(nn.Linear(self.n_a + self.n_d, 2*(self.n_a + self.n_d)))\n",
    "            self.independent_layers.append(ghostBN(2*(self.n_a+self.n_d)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.training == False:\n",
    "            for i in range(self.num_layers_shared):\n",
    "                k = i*2\n",
    "                self.shared_layers[k+1].setTraining(False)\n",
    "            for i in range(self.num_layers_independent):\n",
    "                k = i*2\n",
    "                self.independent_layers[k+1].setTraining(False)\n",
    "                \n",
    "        # Sets the scaling for the residual normalization of the network\n",
    "        residual_scale = torch.sqrt(torch.tensor(.5)).cuda()\n",
    "        x_mem_bool = False\n",
    "        # Iterate through the shared layers\n",
    "        for i in range(self.num_layers_shared):\n",
    "            #each shared layer has two sub layers: a linear and a ghost batch norm, so we iterate 2 at a time\n",
    "            k = i*2\n",
    "            x = self.shared_layers[k+1](self.shared_layers[k](x))\n",
    "            x = F.glu(x, dim = 1)\n",
    "            #check if we have an x in memory\n",
    "            if x_mem_bool:\n",
    "                #add the residual connection, normalizing with the suggested residual scale from the paper\n",
    "                x = (x + x_mem) * residual_scale\n",
    "            #save prior x to memory and set x_mem_bool to true (assuming it isn't already)\n",
    "            x_mem_bool = True\n",
    "            x_mem = x.clone()\n",
    "        \n",
    "        # Iterate through the transformer specific layers\n",
    "        for i in range(self.num_layers_independent):\n",
    "            k = i*2\n",
    "            x = F.glu(self.independent_layers[k+1](self.independent_layers[k](x)), dim = 1)\n",
    "            x = (x + x_mem) * residual_scale\n",
    "            x_mem = x.clone()\n",
    "        return x\n",
    "    \n",
    "    def getSharedLayers(self):\n",
    "        return self.shared_layers\n",
    "    \n",
    "    def setTraining(self, x):\n",
    "        self.training = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Decoder\n",
    "We'll now implement a decoder for self-supervised training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "tabNetDecoder -- defines a decoder for self-supervised training of TabNet as presented here https://arxiv.org/pdf/1908.07442.pdf\n",
    "\n",
    "@initialization: input_dim - the dimension of the output of the encoder, this should be the same dimension as the input to the shared layers in our feature \n",
    "                             encoders\n",
    "                 output_dim - the dimension that our decoder will output, this should be the same dimension as the dataset that we work with\n",
    "                 parameters used in feature transformers: n_a, n_d, shared_layers, num_layers_shared, num_layers_independent\n",
    "                 num_decode_blocks - the number of decoding blocks that we will independently feed the output of the encoder to\n",
    "                 \n",
    "@methods:        forward(self, x): the forward pass for the model\n",
    "                 setS(self, x): set the boolean mask S for selecting which features we are predicting \n",
    "\"\"\"\n",
    "class tabNetDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, n_a, n_d, shared_layers = None, num_layers_shared = 2, num_layers_independent = 2, num_decode_blocks =2):\n",
    "        super(tabNetDecoder, self).__init__()\n",
    "        #initialize params\n",
    "        #-----------------------------\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.S = None\n",
    "        self.n_a = n_a\n",
    "        self.n_d = n_d\n",
    "        self.shared_layers = shared_layers\n",
    "        self.num_layers_shared = num_layers_shared\n",
    "        self.num_layers_independent = num_layers_independent\n",
    "        self.num_decode_blocks = num_decode_blocks\n",
    "        #-----------------------------\n",
    "        \n",
    "        #construct the net:\n",
    "        #The decoder will be composed entirely of feature transformers followed by linear nets\n",
    "        \n",
    "        #make the feature transformers\n",
    "        self.featureTransformers = nn.ModuleList([])\n",
    "        for i in range(self.num_decode_blocks):\n",
    "            self.featureTransformers.append(featTransformer(self.n_a, self.n_d, self.input_dim, \n",
    "                                                            self.shared_layers, self.num_layers_shared, self.num_layers_independent))\n",
    "            \n",
    "        #make the dense layers\n",
    "        self.FCLayers = nn.ModuleList([])\n",
    "        for i in range(self.num_decode_blocks):\n",
    "            self.FCLayers.append(nn.Linear(self.n_a+self.n_d, self.output_dim))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        #make a list to keep track of the copies of the data that we will need, because the input to each feature transformer must be the same data\n",
    "        #so we have to avoid in-place operations being done to the original x\n",
    "        xList = []\n",
    "        #forward prop through each set of feature transformer -> dense\n",
    "        for i in range(self.num_decode_blocks):\n",
    "            #append a copy of x to our list, to be used as our inputs and outputs for the forward prop\n",
    "            xList.append(x.clone())\n",
    "            xList[i] = self.featureTransformers[i](xList[i])\n",
    "            xList[i] = self.FCLayers[i](xList[i])\n",
    "        \n",
    "        #sum all of the outputs of all the layers\n",
    "        first = True\n",
    "        for i in xList:\n",
    "            if first:\n",
    "                summation = i\n",
    "                first = False\n",
    "            else:\n",
    "                summation += i\n",
    "                \n",
    "        #return our final sum with only the features we are trying to predict as non-zeros\n",
    "        return summation*self.S\n",
    "            \n",
    "    def setS(self, x):\n",
    "        self.S = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Full Model\n",
    "We'll now implement a model captable of self-supervision, and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TabNet -- Defines TabNet as described by: https://arxiv.org/pdf/1908.07442.pdf\n",
    "\n",
    "@initialization: input_dim - the number of dimensions in the data\n",
    "                 output_dim - the number of dimensions in the prediction output\n",
    "                 n_a - the number of variables to feed into attention transformers\n",
    "                 n_d - the number of variables to output in the decision step of our feature transformers\n",
    "                 gamma - used in attention transformers\n",
    "                 num_layers_shared - used in feature transformers\n",
    "                 num_layers_independent - used in feature transformers\n",
    "                 num_decode_blocks - the number of feature transformers that our decode step should have\n",
    "                 num_encode_blocks - the number of feature transformer -> attention transformer -> mask blocks we should have\n",
    "                 \n",
    "@methods:        forward(self, x): the forward pass of the model\n",
    "                 setS(self, x): tell the model what features we are predicting in the self-supervised step\n",
    "                 setTraining(self, x): tell our model whether or not we are training\n",
    "                 getMasks(self): returns all of the masks in the current model\n",
    "                 featureAttribution(self): preform feature attribution as described by the paper\n",
    "\"\"\"\n",
    "class TabNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, n_a, n_d, gamma = 1.5, num_layers_shared = 2, num_layers_independent = 2, num_decode_blocks = 4, num_encode_blocks = 4):\n",
    "        super(TabNet, self).__init__()\n",
    "        #initialize params\n",
    "        #-----------------------------\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_a = n_a\n",
    "        self.n_d = n_d\n",
    "        self.num_layers_shared = num_layers_shared\n",
    "        self.num_layers_independent = num_layers_independent\n",
    "        self.num_decode_blocks = num_decode_blocks\n",
    "        self.num_encode_blocks = num_encode_blocks\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.S = None\n",
    "        self.training = True\n",
    "        #-----------------------------\n",
    "        \n",
    "        #BN1 is the normal batch norm that is apply to the input data before it is used by the net\n",
    "        self.BN1 = nn.BatchNorm1d(self.input_dim)\n",
    "        #start by defining our feature transformers for encoding the data\n",
    "        self.enFeatTransformers = nn.ModuleList([])\n",
    "        for i in range(self.num_encode_blocks):\n",
    "            if i == 0:\n",
    "                self.enFeatTransformers.append(featTransformer(self.n_a, self.n_d, self.input_dim, None, self.num_layers_shared, \n",
    "                                                              self.num_layers_independent))\n",
    "                shared_layers = self.enFeatTransformers[i].getSharedLayers()\n",
    "            else:\n",
    "                self.enFeatTransformers.append(featTransformer(self.n_a, self.n_d, self.input_dim, shared_layers, self.num_layers_shared, \n",
    "                                                              self.num_layers_independent))\n",
    "        \n",
    "        #define our attention transformers for the encoded data\n",
    "        self.enAttTransformers = nn.ModuleList([])\n",
    "        for i in range(self.num_encode_blocks):\n",
    "            if i == 0:\n",
    "                self.enAttTransformers.append(attTransformer(self.n_a, self.input_dim, self.gamma))\n",
    "            else:\n",
    "                self.enAttTransformers.append(attTransformer(self.n_a, self.input_dim, self.gamma))\n",
    "                \n",
    "        #define the decoder for use when we are self-supervised training\n",
    "        self.decoder = tabNetDecoder(self.input_dim, self.input_dim, self.n_a, self.n_d, shared_layers, self.num_layers_shared, \n",
    "                                     self.num_layers_independent, self.num_decode_blocks)\n",
    "        \n",
    "        #define two fully connected layers, one for when we are making predictions, and one for when we are in the self-supervised step\n",
    "        self.FCPredict = nn.Linear(self.n_d, self.output_dim)\n",
    "        self.FCSupervised = nn.Linear(self.n_d, self.input_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #start by feeding our input through a basic batch norm layer\n",
    "        x = self.BN1(x)\n",
    "        #initialize a list that will keep track of all of clones of x, this is because we must feed the same data into each step of the model so\n",
    "        #we have to avoid in-place operations\n",
    "        x_list = []\n",
    "        #we will want to keep track of all of the masks for both the regularization parameter defined below, and the attribution step\n",
    "        self.masks = []\n",
    "        #keep track of the decision vector at each point to use in the attribution phase\n",
    "        self.decisions = []\n",
    "        #initialize a vector of all zeros that will keep track of the sum of each of our decisions\n",
    "        finalDecision = torch.zeros(x.shape[0], self.n_d).cuda()\n",
    "        \n",
    "        #check if we are in the self-supervised step of training\n",
    "        if self.S == None:\n",
    "            for i in range(self.num_encode_blocks):\n",
    "                #clone x and only use this clone throughout the propogation\n",
    "                x_list.append(x.clone())\n",
    "                #we need to treat the first and last blocks differently\n",
    "                if i == 0:\n",
    "                    x_list[i] = self.enFeatTransformers[i](x_list[i])\n",
    "                    #we only create a mask for the first layer\n",
    "                    att = x_list[i][:, 0:self.n_a]\n",
    "                    self.masks.append(self.enAttTransformers[i](att))\n",
    "                    #we need to keep track of prior scales as we go\n",
    "                    prior_scales = self.enAttTransformers[i].getPriorScales()\n",
    "                    \n",
    "                    \n",
    "                elif i < self.num_encode_blocks - 1:\n",
    "                    #set the prior scales term based off of the i-1 attention transformer\n",
    "                    self.enAttTransformers[i].setPriorScales(prior_scales)\n",
    "                    #apply the mask\n",
    "                    x_list[i] = x_list[i]*self.masks[i-1]\n",
    "                    x_list[i] = self.enFeatTransformers[i](x_list[i])\n",
    "                    #need both the attention vector and decision vector this time\n",
    "                    att = x_list[i][:, 0:self.n_a]\n",
    "                    dec = F.relu(x_list[i][:,self.n_a:self.n_a+self.n_d])\n",
    "                    #keep track of the decision, and also make it contribute to the final decision\n",
    "                    self.decisions.append(dec)\n",
    "                    finalDecision += dec\n",
    "                    #keep track of the mask\n",
    "                    self.masks.append(self.enAttTransformers[i](att))\n",
    "                    #update prior scales\n",
    "                    prior_scales = self.enAttTransformers[i].getPriorScales()\n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    x_list[i] = x_list[i]*self.masks[i-1]\n",
    "                    x_list[i] = self.enFeatTransformers[i](x_list[i])\n",
    "                    #only need a decision for the last step\n",
    "                    dec = F.relu(x_list[i][:,self.n_a:self.n_a+self.n_d])\n",
    "                    self.decisions.append(dec)\n",
    "                    finalDecision += dec\n",
    "            #check whether to use a softmax or sigmoid for our predictions\n",
    "            if self.output_dim > 1:\n",
    "                finalDecision = torch.softmax(self.FCPredict(finalDecision), dim = 1)\n",
    "            else:\n",
    "                finalDecision = torch.sigmoid(self.FCPredict(finalDecision))\n",
    "        #if we are in the self-supervised section\n",
    "        else:\n",
    "            #forward prop as before\n",
    "            for i in range(self.num_encode_blocks):\n",
    "                x_list.append(x.clone())\n",
    "                if i == 0:\n",
    "                    self.enAttTransformers[i].setS(self.S)\n",
    "                    x_list[i] = self.enFeatTransformers[i](x_list[i])\n",
    "                    att = x_list[i][:, 0:self.n_a]\n",
    "                    self.masks.append(self.enAttTransformers[i](att))\n",
    "                    prior_scales = self.enAttTransformers[i].getPriorScales()\n",
    "                elif i < self.num_encode_blocks - 1:\n",
    "                    self.enAttTransformers[i].setPriorScales(prior_scales)\n",
    "                    self.enAttTransformers[i].setS(self.S)\n",
    "                    x_list[i] = x_list[i]*self.masks[i-1]\n",
    "                    x_list[i] = self.enFeatTransformers[i](x_list[i])\n",
    "                    att = x_list[i][:, 0:self.n_a]\n",
    "                    dec = F.relu(x_list[i][:,self.n_a:self.n_a+self.n_d])\n",
    "                    self.decisions.append(dec)\n",
    "                    finalDecision += dec\n",
    "                    self.masks.append(self.enAttTransformers[i](att))\n",
    "                    self.enAttTransformers[i].setPriorScales(prior_scales)\n",
    "                else:\n",
    "                    self.enAttTransformers[i].setS(self.S)\n",
    "                    x_list[i] = x_list[i]*self.masks[i-1]\n",
    "                    x_list[i] = self.enFeatTransformers[i](x_list[i])\n",
    "                    dec = F.relu(x_list[i][:,self.n_a:self.n_a+self.n_d])\n",
    "                    self.decisions.append(dec)\n",
    "                    finalDecision += dec\n",
    "            finalDecision = self.FCSupervised(finalDecision)\n",
    "            \n",
    "            #decode the decision\n",
    "            self.decoder.setS(self.S)\n",
    "            finalDecision = self.decoder(finalDecision)\n",
    "        return finalDecision\n",
    "    \n",
    "    def setS(self, x):\n",
    "        self.S = x\n",
    "        \n",
    "    def setTraining(self, x):\n",
    "        self.training = x\n",
    "        \n",
    "    def getMasks(self):\n",
    "        return self.masks\n",
    "    \n",
    "    def featureAttribution(self):\n",
    "        v_A = torch.zeros(self.masks[0].shape[0], len(self.decisions)).cuda()\n",
    "        for i in range(len(self.decisions)):\n",
    "            v_A[:,i] = torch.sum(self.decisions[i], dim = 1)\n",
    "        M_sum = torch.zeros(self.masks[0].shape[0], self.input_dim).cuda()\n",
    "        for i in range(len(self.masks)):\n",
    "            if i == len(self.masks)-1:\n",
    "                M_sum[0:self.masks[i].shape[0], :] += torch.mul(self.masks[i], v_A[0:self.masks[i].shape[0],i].view(self.masks[i].shape[0], 1))\n",
    "            else:\n",
    "                M_sum += torch.mul(self.masks[i], v_A[:,i].view(v_A.shape[0], 1))\n",
    "\n",
    "        return torch.mean(M_sum/torch.sum(M_sum, dim = 1).view(self.masks[0].shape[0], 1), dim = 0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model\n",
    "x = torch.rand(128, 10)\n",
    "model = TabNet(x.shape[1], 1, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabNet(\n",
      "  (BN1): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (enFeatTransformers): ModuleList(\n",
      "    (0): featTransformer(\n",
      "      (shared_layers): ModuleList(\n",
      "        (0): Linear(in_features=10, out_features=256, bias=True)\n",
      "        (1): ghostBN(\n",
      "          (GBN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (3): ghostBN(\n",
      "          (GBN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (independent_layers): ModuleList(\n",
      "        (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (1): ghostBN(\n",
      "          (GBN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (3): ghostBN(\n",
      "          (GBN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): featTransformer(\n",
      "      (shared_layers): ModuleList(\n",
      "        (0): Linear(in_features=10, out_features=256, bias=True)\n",
      "        (1): ghostBN(\n",
      "          (GBN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (3): ghostBN(\n",
      "          (GBN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (independent_layers): ModuleList(\n",
      "        (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (1): ghostBN(\n",
      "          (GBN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (3): ghostBN(\n",
      "          (GBN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): featTransformer(\n",
      "      (shared_layers): ModuleList(\n",
      "        (0): Linear(in_features=10, out_features=256, bias=True)\n",
      "        (1): ghostBN(\n",
      "          (GBN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (3): ghostBN(\n",
      "          (GBN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (independent_layers): ModuleList(\n",
      "        (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (1): ghostBN(\n",
      "          (GBN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (3): ghostBN(\n",
      "          (GBN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): featTransformer(\n",
      "      (shared_layers): ModuleList(\n",
      "        (0): Linear(in_features=10, out_features=256, bias=True)\n",
      "        (1): ghostBN(\n",
      "          (GBN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (3): ghostBN(\n",
      "          (GBN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (independent_layers): ModuleList(\n",
      "        (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (1): ghostBN(\n",
      "          (GBN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (3): ghostBN(\n",
      "          (GBN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (enAttTransformers): ModuleList(\n",
      "    (0): attTransformer(\n",
      "      (FC): Linear(in_features=64, out_features=10, bias=True)\n",
      "      (GBN): ghostBN(\n",
      "        (GBN): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): attTransformer(\n",
      "      (FC): Linear(in_features=64, out_features=10, bias=True)\n",
      "      (GBN): ghostBN(\n",
      "        (GBN): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): attTransformer(\n",
      "      (FC): Linear(in_features=64, out_features=10, bias=True)\n",
      "      (GBN): ghostBN(\n",
      "        (GBN): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): attTransformer(\n",
      "      (FC): Linear(in_features=64, out_features=10, bias=True)\n",
      "      (GBN): ghostBN(\n",
      "        (GBN): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): tabNetDecoder(\n",
      "    (shared_layers): ModuleList(\n",
      "      (0): Linear(in_features=10, out_features=256, bias=True)\n",
      "      (1): ghostBN(\n",
      "        (GBN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (3): ghostBN(\n",
      "        (GBN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (featureTransformers): ModuleList(\n",
      "      (0): featTransformer(\n",
      "        (shared_layers): ModuleList(\n",
      "          (0): Linear(in_features=10, out_features=256, bias=True)\n",
      "          (1): ghostBN(\n",
      "            (GBN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (3): ghostBN(\n",
      "            (GBN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (independent_layers): ModuleList(\n",
      "          (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (1): ghostBN(\n",
      "            (GBN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (3): ghostBN(\n",
      "            (GBN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): featTransformer(\n",
      "        (shared_layers): ModuleList(\n",
      "          (0): Linear(in_features=10, out_features=256, bias=True)\n",
      "          (1): ghostBN(\n",
      "            (GBN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (3): ghostBN(\n",
      "            (GBN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (independent_layers): ModuleList(\n",
      "          (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (1): ghostBN(\n",
      "            (GBN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (3): ghostBN(\n",
      "            (GBN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): featTransformer(\n",
      "        (shared_layers): ModuleList(\n",
      "          (0): Linear(in_features=10, out_features=256, bias=True)\n",
      "          (1): ghostBN(\n",
      "            (GBN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (3): ghostBN(\n",
      "            (GBN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (independent_layers): ModuleList(\n",
      "          (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (1): ghostBN(\n",
      "            (GBN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (3): ghostBN(\n",
      "            (GBN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): featTransformer(\n",
      "        (shared_layers): ModuleList(\n",
      "          (0): Linear(in_features=10, out_features=256, bias=True)\n",
      "          (1): ghostBN(\n",
      "            (GBN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (3): ghostBN(\n",
      "            (GBN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (independent_layers): ModuleList(\n",
      "          (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (1): ghostBN(\n",
      "            (GBN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (3): ghostBN(\n",
      "            (GBN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (FCLayers): ModuleList(\n",
      "      (0): Linear(in_features=128, out_features=10, bias=True)\n",
      "      (1): Linear(in_features=128, out_features=10, bias=True)\n",
      "      (2): Linear(in_features=128, out_features=10, bias=True)\n",
      "      (3): Linear(in_features=128, out_features=10, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (FCPredict): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (FCSupervised): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0742, 0.0960, 0.1411, 0.1233, 0.1238, 0.0774, 0.0892, 0.1241, 0.0688,\n",
       "        0.0820], device='cuda:0', grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.clone().detach().requires_grad_(True).cuda()\n",
    "model.cuda()\n",
    "model(x)\n",
    "model.featureAttribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verifying our Model on the Cover-Type Dataset\n",
    "This data set is used in the paper, so we can test to see if our model is coded correctly. This data can be found here: https://www.kaggle.com/uciml/forest-cover-type-dataset and the task is to guess what type of forest cover a particular forest has given things like its elevation. Coding this: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data = pd.read_csv(\"./Data/covtype.csv\")\n",
    "train_y = train_data['Cover_Type']\n",
    "train_x = train_data.drop(columns=['Cover_Type'])\n",
    "train_y = train_y.to_numpy()\n",
    "train_x = train_x.to_numpy()\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_x, train_y, test_size=0.1, random_state=4)\n",
    "train_x = torch.from_numpy(train_x)\n",
    "test_x = torch.from_numpy(test_x)\n",
    "train_y = torch.from_numpy(train_y)\n",
    "test_y = torch.from_numpy(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in order to make use of self-supervisesd training we can write a quick function to generate a binary mask of features that we would like our model to predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generates a binary mask for self-supervised learning of size batch_sizeXnum_features, where each element i,j in the mask is a 1 with prob p and 0 otherwise\n",
    "def generateS(batch_size, num_features, p):\n",
    "    S = torch.zeros(batch_size, num_features)\n",
    "    for i in range(S.shape[0]):\n",
    "        for j in range(S.shape[1]):\n",
    "            rand = torch.rand(1,1)\n",
    "            if rand < p:\n",
    "                S[i,j] = 1\n",
    "    return S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to discuss some special aspects of the loss functions employed for training this model.\n",
    "#### Implementing Loss Functions for TabNet\n",
    "For the prediction phase the authors recommend using a regularization factor that will encourage sparsity in the masks created by the attention transformers. This is defined as follows:\n",
    "$$\n",
    "\\lambda = \\sum_{i=1}^{N_{steps}}\\sum_{b=1}^B\\sum_{j=1}^D-\\frac{M_{b,j}[i]}{N_{steps}*B}*log(M_{b,j}[i]+\\epsilon)\n",
    "$$\n",
    "Where $N_{steps}$ is the number of 'blocks' in TabNet, $B$ is the batch size, $D$ is the number of columns in our dataset, $M_{b,j}[i]$ is the (b,j)th element of the ith mask in the model, and $\\epsilon$ is a small fixed value that is set so that the log is never $0$. This is the same as multiplying each mask together with it's log + epsilon, then summing all of the elements of the resulting matrix. This is added with normal cross entropy in the prediction case, and with a different loss function that we will define below for the self supervised learning. Implementing this in Pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the regularizing parameter to encourage sparsity in the masks\n",
    "#M is a list of all the masks in the model (returned by model.getMasks()), N is the number of encoding blocks in our model, and B is the batch size\n",
    "def regularizingParam(M, N, B):\n",
    "    epsilon = 10**-6\n",
    "    summation = torch.zeros(1,1).cuda()\n",
    "    for i in range(len(M)):\n",
    "        #the last mask might have a different shape if the data set is not divisible by the batch size\n",
    "        if i == len(M)-1:\n",
    "            summation += torch.sum(-1*M[i]*torch.log(M[i]+epsilon)) * 1/(N*M[i].shape[0])\n",
    "        else:\n",
    "            summation += torch.sum(-1*M[i]*torch.log(M[i]+epsilon)) * 1/(N*B)\n",
    "    return summation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, there is a special loss function for the self-supervised learning step of training. This function looks like this: \n",
    "$$\n",
    "loss = \\sum_{b=1}^B \\sum_{j=1}^D|(\\hat{f}_{b,j}-f_{b,j})*S/\\sqrt{\\sum_{b=1}^B(f_{b,j}-1/B*\\sum_{b=1}^Bf_{b,j})^2}|^2\n",
    "$$\n",
    "Where $\\hat{f}_{b,j}$ is the (b,j)th element of our predicted output, $f_{b,j}$ is the (b,j)th element of the original dataset, and S is the binary mask that selects what features our system was supposed to predict (all other variables defined as in the regularization constant). Implementing this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finds the supervised loss as described above, f is the real features f_hat is the predictions from our model, and S is the binary mask created for \n",
    "#self-supervised learning\n",
    "def selfSupervisedLoss(f, f_hat, S):\n",
    "    B = f.shape[0]\n",
    "    D = f.shape[1]\n",
    "    regularizingPart = torch.std(f, dim = 0).view(1,D)\n",
    "    num = (f_hat-f)*S\n",
    "    return torch.sum((num/regularizingPart)**2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write our training loops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10,     1] loss: 2209734.000\n",
      "[10,     2] loss: 1729758.750\n",
      "[10,     3] loss: 1418680.000\n",
      "[10,     4] loss: 1159083.250\n",
      "[10,     5] loss: 839506.938\n",
      "Done Training\n"
     ]
    }
   ],
   "source": [
    "#The number of epochs for each feature mask S\n",
    "epochs = 10\n",
    "#The number of feature masks to train\n",
    "num_self_supervised = 5\n",
    "#batch size for the model, this would normally be higher, but since we're dealing with such small data I thought goign lower would be fine\n",
    "batch_size = 2**12\n",
    "#define the model\n",
    "#remove columns with 0 standard deviation\n",
    "train_x = torch.cat((train_x[:,0:20], train_x[:,21:28], train_x[:,29:train_x.shape[1]]), dim = 1)\n",
    "test_x = torch.cat((test_x[:,0:20], test_x[:,21:28], test_x[:,29:test_x.shape[1]]), dim = 1)\n",
    "model = TabNet(train_x.shape[1], 8, 64, 64)\n",
    "model.cuda()\n",
    "#set the learning rate and optimizer\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "train_x = train_x.cuda()\n",
    "train_y = train_y.cuda()\n",
    "\n",
    "for i in range(num_self_supervised):\n",
    "    #Generate the binary mask S\n",
    "    S = generateS(batch_size, train_x.shape[1], .15).cuda()\n",
    "    #set S in the model\n",
    "    model.setS(S)\n",
    "    #find the number of complete batches we can use\n",
    "    num_batches = int(np.floor(train_x.shape[0]/(100*batch_size)))\n",
    "    for epoch in range(epochs):\n",
    "        #keep track of the running_loss\n",
    "        running_loss = 0\n",
    "        for j in range(num_batches):\n",
    "            start = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            #our input is multipled by 1-S so our model actually needs to guess other features\n",
    "            input = (1-S)*train_x[j*batch_size:(j+1)*batch_size,:]\n",
    "            output = model(input.float())\n",
    "            reg = regularizingParam(model.getMasks(), 2, batch_size)\n",
    "            lambdaSparse = 0.0001\n",
    "            loss = selfSupervisedLoss(input, output, S) + reg*lambdaSparse\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if epoch == 0 and j ==1:\n",
    "                print(\"Time for one batch: \" + str(time.time() - start))\n",
    "            #keep track of the running loss\n",
    "            running_loss += loss.item()\n",
    "            if j == num_batches-1 and epoch == epochs - 1:\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / num_batches))\n",
    "        \n",
    "print(\"Done Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   127] loss: 2.048\n",
      "[2,   127] loss: 1.993\n",
      "[3,   127] loss: 1.952\n",
      "[4,   127] loss: 1.922\n",
      "[5,   127] loss: 1.899\n",
      "[6,   127] loss: 1.882\n",
      "[7,   127] loss: 1.867\n",
      "[8,   127] loss: 1.856\n",
      "[9,   127] loss: 1.847\n",
      "[10,   127] loss: 1.839\n",
      "Done Training\n"
     ]
    }
   ],
   "source": [
    "model.setS(None)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 10\n",
    "num_batches = int(np.floor(train_x.shape[0]/(batch_size)))\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    for j in range(num_batches):\n",
    "        optimizer.zero_grad()\n",
    "        input = train_x[j*batch_size:(j+1)*batch_size,:]\n",
    "        target = train_y[j*batch_size:(j+1)*batch_size]\n",
    "        output = model(input.float())\n",
    "        reg = regularizingParam(model.getMasks(), 2, batch_size)\n",
    "        lambdaSparse = 0.0001\n",
    "        loss = criterion(output, target.long()) + reg*lambdaSparse\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if j == num_batches-1:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "              (epoch + 1, j + 1, running_loss / num_batches))\n",
    "\n",
    "print(\"Done Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0177, 0.0323, 0.0214, 0.0321, 0.0218, 0.0272, 0.0230, 0.0272, 0.0218,\n",
       "        0.0187, 0.0240, 0.0232, 0.0222, 0.0247, 0.0151, 0.0193, 0.0147, 0.0177,\n",
       "        0.0106, 0.0132, 0.0198, 0.0158, 0.0241, 0.0142, 0.0231, 0.0364, 0.0147,\n",
       "        0.0110, 0.0168, 0.0108, 0.0125, 0.0137, 0.0136, 0.0185, 0.0140, 0.0158,\n",
       "        0.0092, 0.0203, 0.0217, 0.0134, 0.0364, 0.0199, 0.0128, 0.0298, 0.0244,\n",
       "        0.0127, 0.0153, 0.0114, 0.0127, 0.0158, 0.0221, 0.0194],\n",
       "       device='cuda:0', grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.featureAttribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss on the test set was: \n",
      "1.8382829427719116\n"
     ]
    }
   ],
   "source": [
    "test_x = test_x[0:batch_size,:].cuda()\n",
    "test_y = test_y[0:batch_size].cuda()\n",
    "output = model(test_x.float())\n",
    "loss = criterion(output.float(), test_y.long())\n",
    "print(\"The loss on the test set was: \")\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the test set was:\n",
      "0.479736328125\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "for i in range(output.shape[0]):\n",
    "    if torch.argmax(output[i,:]) == test_y[i]:\n",
    "        acc += 1\n",
    "print(\"The accuracy on the test set was:\")\n",
    "print(acc/output.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check my work here is an implementation of TabNet in Pytorch through the \"pytorch_tabnet\" library. Instructions for installing this library as well as the documentation for the library can be found here: https://github.com/dreamquark-ai/tabnet. In general their implementation seems to be faster, but with fewer perks, for instance I don't think they have the ability to do the self-supervised training, or the feature attribution that I have implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 10 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.69326 |  0.72803 |   28.0      \n",
      "| 2     | 0.73922 |  0.75415 |   57.8      \n",
      "| 3     | 0.75550 |  0.75415 |   87.4      \n",
      "| 4     | 0.76345 |  0.77441 |   117.0     \n",
      "| 5     | 0.77147 |  0.77319 |   146.9     \n",
      "| 6     | 0.78137 |  0.79053 |   176.7     \n",
      "| 7     | 0.78487 |  0.79272 |   207.6     \n",
      "| 8     | 0.78862 |  0.77588 |   238.5     \n",
      "| 9     | 0.78408 |  0.79883 |   269.2     \n",
      "| 10    | 0.78244 |  0.78394 |   301.0     \n",
      "Training done in 300.965 seconds.\n",
      "---------------------------------------\n",
      "Accuracy: 79.8828125\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "train_x = train_x.cpu().numpy()\n",
    "train_y = train_y.cpu().numpy()\n",
    "test_x = test_x.cpu().numpy()\n",
    "test_y = test_y.cpu().numpy()\n",
    "clf = TabNetClassifier()  #TabNetRegressor() is the other one\n",
    "clf.fit(train_x, train_y, test_x, test_y, max_epochs = 10)\n",
    "preds = clf.predict(test_x)\n",
    "predictions = [round(value) for value in preds]\n",
    "accuracy = accuracy_score(test_y, predictions)\n",
    "print(\"Accuracy: \" + str(accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think the problem here lies in the self-supervised training step. Some of the columns of this dataset are one-hot-encoded with very small standard deviations, so the loss is very large. I am curious to see if my model preforms better without this step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = torch.from_numpy(test_x).cuda()\n",
    "test_y = torch.from_numpy(test_y).cuda()\n",
    "train_x = torch.from_numpy(train_x).cuda()\n",
    "train_y = torch.from_numpy(train_y).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   127] loss: 1.958\n",
      "[2,   127] loss: 1.703\n",
      "[3,   127] loss: 1.618\n",
      "[4,   127] loss: 1.591\n",
      "[5,   127] loss: 1.578\n",
      "[6,   127] loss: 1.571\n",
      "[7,   127] loss: 1.565\n",
      "[8,   127] loss: 1.561\n",
      "[9,   127] loss: 1.556\n",
      "[10,   127] loss: 1.552\n",
      "Done Training\n"
     ]
    }
   ],
   "source": [
    "model.setS(None)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 10\n",
    "num_batches = int(np.floor(train_x.shape[0]/(batch_size)))\n",
    "model = TabNet(train_x.shape[1], 8, 64, 64)\n",
    "model.cuda()\n",
    "#set the learning rate and optimizer\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    for j in range(num_batches):\n",
    "        optimizer.zero_grad()\n",
    "        input = train_x[j*batch_size:(j+1)*batch_size,:]\n",
    "        target = train_y[j*batch_size:(j+1)*batch_size]\n",
    "        output = model(input.float())\n",
    "        reg = regularizingParam(model.getMasks(), 2, batch_size)\n",
    "        lambdaSparse = 0.0001\n",
    "        loss = criterion(output, target.long()) + reg*lambdaSparse\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if j == num_batches-1:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "              (epoch + 1, j + 1, running_loss / num_batches))\n",
    "\n",
    "print(\"Done Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss on the test set was: \n",
      "1.5583769083023071\n"
     ]
    }
   ],
   "source": [
    "test_x = test_x[0:batch_size,:].cuda()\n",
    "test_y = test_y[0:batch_size].cuda()\n",
    "output = model(test_x.float())\n",
    "loss = criterion(output.float(), test_y.long())\n",
    "print(\"The loss on the test set was: \")\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the test set was:\n",
      "0.718505859375\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "for i in range(output.shape[0]):\n",
    "    if torch.argmax(output[i,:]) == test_y[i]:\n",
    "        acc += 1\n",
    "print(\"The accuracy on the test set was:\")\n",
    "print(acc/output.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Thoughts\n",
    "This accuracy is not quite as high as what we would expect from the library implementation, but I believe I've identified a couple of explanations for this discrepancy. First, the paper mentions that they use an automated method to select hyper parameters for their model which helped with their accuracy. It appears that the library I was using to compare my method to also uses some strange hyper-parameter defaults for their training (for instance, they use a non-standard momentum for the batch-normalization). Since I did not implement these I would expect to get a lower accuracy as my model does not have optimized hyperparameters. Second, the default size of the neural net for the library implementation is actually larger than mine (it has more decision steps). I would try changing this for my model, but due to inefficiencies in my code that the library implementation has found solutions to I am unable to train a larger neural network on this data, and since my end accuracy was only 8% lower than theirs I believe that my implementation is working, just not as well or as fast of the library version. A few notes: I have assumed that the data that is fed into the model will be divisible by the batch size. This is because our attention transformers create masks of a given size, and the prior scales function depends on that dimension. There are ways around this, but the paper doesn't mention a solution for this particular problem, and I didn't want to try anything incorrect for the implementation. That said, I did put something into the code where it might run with a changing batch size, but it does break the math a little (though I did not test this extensively). Overall, this was a great exercise for me, and I learned a lot by doing it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addressing Feedback For My Prior Biweekly Report\n",
    "### Using CIFAR-10\n",
    "To get a feel for this dataset I'll just implement the small neural net I implemented during the Pytorch tutorial during my week 1 progress report. I'll be using CIFAR-10 which only has 10 classes, but they are still much more complex than the MNIST database. This data can be found through the University of Toronto: https://www.cs.toronto.edu/~kriz/cifar.html. First we'll read the data into python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is a GPU available?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(\"Is a GPU available?\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The keys in our data dictionary:\n",
      "dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "The shape of the data:\n",
      "(10000, 3072)\n",
      "The shape of the labels:\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "#code for this part from https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "data = unpickle(\"./Data/cifar-10-batches-py/data_batch_1\")\n",
    "\n",
    "#Print some basics about the data:\n",
    "print(\"The keys in our data dictionary:\")\n",
    "print(data.keys())\n",
    "print(\"The shape of the data:\")\n",
    "print(data[b'data'].shape)\n",
    "print(\"The shape of the labels:\")\n",
    "print(len(data[b'labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training data's shape:\n",
      "torch.Size([10000, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "#now, from the documentation we know that each image is a 32x32x3 image, and we can see that these have been flattened into a single array.\n",
    "#we'll need to change this into something pytorch can work with for convolutional nets\n",
    "\n",
    "train_x = torch.from_numpy(data[b'data'])\n",
    "train_x = train_x.view(-1,3,32,32)\n",
    "print(\"Our training data's shape:\")\n",
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now, split this into test and training data:\n",
    "train_x = train_x.numpy()\n",
    "train_y = np.array(data[b'labels'])\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_x, train_y, test_size=0.1, random_state=4)\n",
    "train_x = torch.from_numpy(train_x)\n",
    "train_y = torch.from_numpy(train_y)\n",
    "test_x = torch.from_numpy(test_x)\n",
    "test_y = torch.from_numpy(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#define our neural net:\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        self.fc1 = nn.Linear(576, 120)  \n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x), dim = 1)\n",
    "        return x\n",
    "    #Gets the number of features after all of the dimensionality reduction is applied to x from the convolutions and maxpoolings\n",
    "    def num_flat_features(self, x):\n",
    "        # x's shape is originally nSamples, nChannels, Height, Width for images, so we want to compute nChannels * num features = nChannels * Current Height * Current Width\n",
    "        # (current referring to after conv and pooling)\n",
    "        size = x.shape[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "net = Net()\n",
    "net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   141] loss: 2.281\n",
      "[2,   141] loss: 2.206\n",
      "[3,   141] loss: 2.162\n",
      "[4,   141] loss: 2.132\n",
      "[5,   141] loss: 2.111\n",
      "[6,   141] loss: 2.090\n",
      "[7,   141] loss: 2.078\n",
      "[8,   141] loss: 2.070\n",
      "[9,   141] loss: 2.050\n",
      "[10,   141] loss: 2.043\n",
      "[11,   141] loss: 2.030\n",
      "[12,   141] loss: 2.020\n",
      "[13,   141] loss: 2.013\n",
      "[14,   141] loss: 2.004\n",
      "[15,   141] loss: 1.995\n",
      "[16,   141] loss: 1.993\n",
      "[17,   141] loss: 1.983\n",
      "[18,   141] loss: 1.980\n",
      "[19,   141] loss: 1.971\n",
      "[20,   141] loss: 1.963\n",
      "[21,   141] loss: 1.964\n",
      "[22,   141] loss: 1.951\n",
      "[23,   141] loss: 1.948\n",
      "[24,   141] loss: 1.942\n",
      "[25,   141] loss: 1.932\n",
      "[26,   141] loss: 1.933\n",
      "[27,   141] loss: 1.928\n",
      "[28,   141] loss: 1.920\n",
      "[29,   141] loss: 1.919\n",
      "[30,   141] loss: 1.912\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "#we will use the adam optimizer for this\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "#Train the model:\n",
    "#The number of epochs to do on the whole dataset\n",
    "epochs = 30\n",
    "batch_size = 64\n",
    "batchesPerEpoch = int(np.ceil(train_x.shape[0]/batch_size))\n",
    "for epoch in range(epochs):\n",
    "    #running_loss (from the pytorch documentation) will keep track of the average loss as we train\n",
    "    running_loss = 0.0\n",
    "    temp = torch.randperm(train_x.shape[0])\n",
    "    train_x = train_x[temp, :, :, :]\n",
    "    train_y = train_y[temp]\n",
    "    for i in range(batchesPerEpoch):\n",
    "        #set the gradients to zero (or else we'll just keep adding to gradients)\n",
    "        optimizer.zero_grad()\n",
    "        #create the input image constructed from a single image, we have to reshape it (using veiw()) to make it usable by our neural network\n",
    "        if i == batchesPerEpoch - 1:\n",
    "            input = train_x[i*batch_size:train_x.shape[0],:,:,:]\n",
    "            target = train_y[i*batch_size:train_y.shape[0]]\n",
    "        else:\n",
    "            input = train_x[i*batch_size:(i+1)*batch_size,:,:,:]\n",
    "            target = train_y[i*batch_size:(i+1)*batch_size]\n",
    "        input = input.cuda()\n",
    "        target = target.cuda()\n",
    "        outputs = net(input.float())\n",
    "        #our loss is calculated using cross entropy, we reshape train_y[i] here because it needs to have the same first dimension as outputs\n",
    "        loss = criterion(outputs, target.long())\n",
    "        #calculate the gradients and take a step using the optimizer\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i == batchesPerEpoch-1:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / batchesPerEpoch))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss on the test set was: 2.002727720499039\n",
      "The accuracy on the test set was: 0.456\n"
     ]
    }
   ],
   "source": [
    "predicted = np.zeros((test_x.shape[0], 10))\n",
    "running_loss = 0\n",
    "for i in range(test_x.shape[0]):\n",
    "    input = test_x[i,:,:,:].view(1,3,32,32).cuda()\n",
    "    outputs = net(input.float())\n",
    "    \n",
    "    loss = criterion(outputs, test_y[i].view(1).cuda().long())\n",
    "    predicted[i,:] = outputs.cpu().detach().numpy()\n",
    "    running_loss += loss.item()\n",
    "print(\"The loss on the test set was: \" + str(running_loss/test_x.shape[0]))\n",
    "correct = 0\n",
    "for i in range(predicted.shape[0]):\n",
    "    pred = np.argmax(predicted[i,:])\n",
    "    if int(pred) == int(test_y[i]):\n",
    "        correct += 1\n",
    "print(\"The accuracy on the test set was: \" + str(correct/predicted.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Thoughts on CIFAR-10\n",
    "This accuracy certainly isn't that good, but given how small the neural net was that we used was, and how we are only using a single batch of the training data I think that it is good enough. I now know how to load in and structure the CIFAR-10 data set for any future projects, and I expect that I will use this in favor of MNIST from now on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GANs and Mode Collapse\n",
    "In your feedback you mentioned 'mode collapse' was possible if I were to use the complete digit data. Before I get into an implementation using the complete MNIST dataset I will summarize a course from Google on the topic: https://developers.google.com/machine-learning/gan/problems. Mode collapse is the tendency for the generator to find a single optimal output and always give that output. This is possible if the discriminator becomes stuck in a local minima, making it impossible for it to learn how to combat the generator's strategy. This course says that using Wasserstein loss is already a step in the right direction for combating this problem, so I'll try running what I've already coded on the complete data set and see if it works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainData = pd.read_csv(\"./MNIST_train.csv\")\n",
    "train_y = trainData['label'].to_numpy()\n",
    "train_x = trainData.to_numpy()\n",
    "#The first column is the label column so we will remove that\n",
    "train_x = train_x[:,1:train_x.shape[1]]\n",
    "#turn our data into torch tensors\n",
    "train_y = torch.from_numpy(train_y)\n",
    "train_x = torch.from_numpy(train_x)\n",
    "#We now need to reshape train_x so that it is in the correct format for the image classifier\n",
    "train_x = train_x.view(train_x.shape[0], 1, int(np.sqrt(train_x.shape[1])), int(np.sqrt(train_x.shape[1])))\n",
    "\n",
    "#Now we'll look an image and write a function that will take a tensor and show the plot of the image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def TImShow(image):\n",
    "    image = image[0,:,:]\n",
    "    plt.imshow(image.cpu().detach().numpy())\n",
    "    plt.show()\n",
    "TImShow(train_x[1,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (batchNorm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (batchNorm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (leReLU): LeakyReLU(negative_slope=0.2)\n",
      "  (fc1): Linear(in_features=3136, out_features=1, bias=True)\n",
      ")\n",
      "Generator(\n",
      "  (leReLU): LeakyReLU(negative_slope=0.2)\n",
      "  (fc1): Linear(in_features=100, out_features=6272, bias=True)\n",
      "  (tConv1): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (batchNorm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tConv2): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (batchNorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv1): Conv2d(128, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# For reproducability\n",
    "torch.manual_seed(1)\n",
    "# Define the critic/discriminator model\n",
    "class Discriminator(nn.Module):\n",
    "    #Note that exImg will contain and example image which we can get the dimension of NOTE: this should have dimension (1, 1, height, width)\n",
    "    #batchSize will be the size of the batches\n",
    "    def __init__(self, exImg):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # The article uses same padding, which is not trivial to implement in pytorch. We will use the padding function with \n",
    "        #p = (filter size - 1)/2 = 1 to get same padding\n",
    "        self.inputImg = exImg\n",
    "        self.conv1 = nn.Conv2d(1, 64, 3, stride = 2, padding = 1)\n",
    "        self.batchNorm1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, stride = 2, padding = 1)\n",
    "        self.batchNorm2 = nn.BatchNorm2d(64)\n",
    "        self.leReLU = nn.LeakyReLU(.2)\n",
    "        self.fc1 = nn.Linear(3136, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leReLU(self.conv1(x))\n",
    "        x = self.batchNorm1(x)\n",
    "        x = self.leReLU(self.conv2(x))\n",
    "        x = self.batchNorm2(x)\n",
    "        # Remember that view reshapes the tensor, so this is a flattening layer since it contains a -1 we will get an output shape of\n",
    "        # (nSamples, nChannels * number of features (which has changed from the original due to the convolutions and pooling))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    #Gets the number of features after all of the dimensionality reduction is applied to x from the convolutions and maxpoolings\n",
    "    def num_flat_features(self, x):\n",
    "        # x's shape is originally nSamples, nChannels, Height, Width for images, so we want to compute nChannels * num features = \n",
    "        # nChannels * Current Height * Current Width (current referring to after conv and pooling)\n",
    "        size = x.shape[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "#load full data into python, no need to split into test and train since this is not about accuracy\n",
    "train_y = trainData['label'].to_numpy()\n",
    "train_x = trainData.to_numpy()\n",
    "train_x = train_x[:,1:train_x.shape[1]]\n",
    "#turn our data into torch tensors\n",
    "train_y = torch.from_numpy(train_y)\n",
    "train_x = torch.from_numpy(train_x)\n",
    "#We now need to reshape train_x so that it is in the correct format for the image classifier\n",
    "train_x = train_x.view(train_x.shape[0], 1, int(np.sqrt(train_x.shape[1])), int(np.sqrt(train_x.shape[1])))\n",
    "batch_size = 32\n",
    "exImg = train_x[0,:,:,:].view(1,1,28,28)\n",
    "dis = Discriminator(exImg)\n",
    "dis.cuda()\n",
    "print(dis)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.leReLU = nn.LeakyReLU(.2)\n",
    "        #Start with a 100 variable input from the latent space, and turn it into 7x7x128 linear vector (we'll reshape this next in the forward pass)\n",
    "        self.fc1 = nn.Linear(100, 7*7*128)\n",
    "        #next we need to upsample with a transposed convolution\n",
    "        self.tConv1 = nn.ConvTranspose2d(128, 128, 4, stride = 2, padding = 1)\n",
    "        self.batchNorm1 = nn.BatchNorm2d(128)\n",
    "        self.tConv2 = nn.ConvTranspose2d(128, 128, 4, stride = 2, padding = 1)\n",
    "        self.batchNorm2 = nn.BatchNorm2d(128)\n",
    "        self.conv1 = nn.Conv2d(128, 1, 7, padding = 3)\n",
    "        #this must have output dimension equal to the image\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.leReLU(self.fc1(x))\n",
    "        x = x.view(-1,128,7,7) #working with 7x7 images\n",
    "        x = self.leReLU(self.batchNorm1(self.tConv1(x))) #upscale to 14x14\n",
    "        x = self.leReLU(self.batchNorm2(self.tConv2(x))) #upscale to 28x28\n",
    "        x = torch.tanh(self.conv1(x))\n",
    "        return x\n",
    "gen = Generator()\n",
    "gen.cuda()\n",
    "print(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The generator's first attempt at generating an image: \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20,   131] loss: 10.596\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40,   131] loss: 9.979\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAANAUlEQVR4nO3dX4xU93nG8edh2cUOxOoSCkKAEsehTa0qxdWKWnHUunUbO1wU+8KtuYioZYlcYMlRLbVWWsnuVa3GiRWpkVNSo9DEtevWsYxUKwlFqWhaB3mNKeBQB2IRjEEsDnWD4/Bn2bcXe1wteOc3y8yZOaO+3480mpnzztnzasTDOXN+Z+bniBCA///mNd0AgP4g7EAShB1IgrADSRB2IIn5/dzYiK+Kq72wZZ2RAaA7Z/UznY9znq3WVdht3ybpS5KGJP1tRDxcev3VXqgbr1rXsj519mw37QDp7Y6dLWsdH8bbHpL0ZUmfknS9pA22r+/07wHorW4+s6+VdDgiXouI85KekrS+nrYA1K2bsK+Q9PqM58eqZZewvcn2uO3x8zrXxeYAdKObsM92EuA9Z9giYktEjEXE2IgWdLE5AN3oJuzHJK2a8XylpOPdtQOgV7oJ+4uSVtu+1vaIpLskba+nLQB163joLSImbd8r6duaHnrbGhGvtFlHU+cvtH7BvKE2G50q/fHyukByXY2zR8Tzkp6vqRcAPcTlskAShB1IgrADSRB2IAnCDiRB2IEk+vp9dlnyvFm/aitJiqk2Y+WMpQMdY88OJEHYgSQIO5AEYQeSIOxAEoQdSKK/Q28hxeRkXzcJYBp7diAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiiv99nRzpDo6Mta29+Y0lx3cV/PlKsx8vFGcJxma7CbvuIpDOSLkqajIixOpoCUL869uy/HRFv1vB3APQQn9mBJLoNe0j6ju2XbG+a7QW2N9ketz1+Qee63ByATnV7GH9TRBy3vVTSDtv/FRG7Zr4gIrZI2iJJ13gxk7UBDelqzx4Rx6v7CUnPSlpbR1MA6tdx2G0vtP3+dx9L+qSkA3U1BqBe3RzGL5P0rO13/87fR8S3aukKA2P+yhXF+j9+/9li/X3zWo+VX4yp4rr/+k/Dxfrnf/mGYp05Ci7Vcdgj4jVJv1ZjLwB6iKE3IAnCDiRB2IEkCDuQBGEHkuArrtlND5229Oi/P12sv2/ewmK9NLz2o8mfF9ddPfxOsa41Hy3Xx7nsYyb27EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBOPsyZ2/tfyDwGemdhfr33pnQbG+efvdLWtTCy8W1/3S7zxRrPvVHxfr/CzSpdizA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASjLMn98hjXy7Wv37648X6Dzd8sFhffc3bLWuf2DpeXPfGq04V618ZYl91JXi3gCQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJxtmT+4ujv1+s7z+8sli//tzJYn3+I61/+33z6J7iuq9NlqdsvvjW/xTruFTbPbvtrbYnbB+YsWyx7R22D1X3o71tE0C35nIY/zVJt1227AFJOyNitaSd1XMAA6xt2CNil6TTly1eL2lb9XibpNvrbQtA3To9QbcsIk5IUnW/tNULbW+yPW57/ILOdbg5AN3q+dn4iNgSEWMRMTas8o8TAuidTsN+0vZySaruJ+prCUAvdBr27ZI2Vo83SnqunnYA9ErbcXbbT0q6WdIS28ckPSjpYUlP275H0lFJd/aySfTO4VNLivWRE+Wx7qN/uKpYf+baz7esXTPv6uK6d/3D5mL9w3qhWMel2oY9Ija0KN1Scy8AeojLZYEkCDuQBGEHkiDsQBKEHUiCr7gmt2r0rWL97jv/uVj/ratfL9aXz1/UsnZssvXPTEvSRx56uVifKlZxOfbsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AE4+zJ3brsB8X6nYt+UqwPufU4ejt3H2r1hcpp886Wx/BxZdizA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASjLMn9y+3fKRY/93d5XH4j41c1fG2P/YLbxTrB4pVXCn27EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBOPsyV08OVGsb/7j+4r1f/vrv+l423v/e2WxPl9HO/7beK+2e3bbW21P2D4wY9lDtt+wvbe6rettmwC6NZfD+K9Jum2W5Y9GxJrq9ny9bQGoW9uwR8QuSaf70AuAHurmBN29tvdVh/mjrV5ke5PtcdvjF3Sui80B6EanYX9M0nWS1kg6IekLrV4YEVsiYiwixoa1oMPNAehWR2GPiJMRcTEipiR9VdLaetsCULeOwm57+Yynd4hvIwIDr+04u+0nJd0saYntY5IelHSz7TWSQtIRSZ/pXYto0qJv9+7/8dEF7xTrZ3q25Zzahj0iZvsl/8d70AuAHuJyWSAJwg4kQdiBJAg7kARhB5LgK64omvr52WL9Qlws1l84N9SytmffdcV1V+vNYh1Xhj07kARhB5Ig7EAShB1IgrADSRB2IAnCDiTBODuKPNR6nFyShl2u/8aCCy1ro/vY1/QT7zaQBGEHkiDsQBKEHUiCsANJEHYgCcIOJME4O4o8MtzV+qcutp7ya+k39hXXnepqy7gce3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIJxdhR5ZKRYb/e78bd+5U9a1lb+7D866gmdabtnt73K9ndtH7T9iu37quWLbe+wfai6H+19uwA6NZfD+ElJ90fEr0i6UdJm29dLekDSzohYLWln9RzAgGob9og4ERF7qsdnJB2UtELSeknbqpdtk3R7j3oEUIMrOkFn+0OSbpC0W9KyiDghTf+HIGlpi3U22R63PX5Bra+TBtBbcw677UWSnpH02Yj46VzXi4gtETEWEWPDWtBJjwBqMKew2x7WdNCfiIhvVotP2l5e1ZdLmuhNiwDq0HbozbYlPS7pYER8cUZpu6SNkh6u7p/rSYdo1Kk7PlqsPzjxk2J95V++UGc76MJcxtlvkvRpSftt762WfU7TIX/a9j2Sjkq6sycdAqhF27BHxPckuUX5lnrbAdArXC4LJEHYgSQIO5AEYQeSIOxAEnzFFUUfeLl8seRTuz5erK+O79fZDrrAnh1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkmCcPbt5Q8Xy0Km3ivVfuv/VYj2utB/0DHt2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCcfbspspTLk8ee6NPjaDX2LMDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBJtw257le3v2j5o+xXb91XLH7L9hu291W1d79sF0Km5XFQzKen+iNhj+/2SXrK9o6o9GhGP9K49AHWZy/zsJySdqB6fsX1Q0opeNwagXlf0md32hyTdIGl3tehe2/tsb7U92mKdTbbHbY9f0LnuugXQsTmH3fYiSc9I+mxE/FTSY5Kuk7RG03v+L8y2XkRsiYixiBgb1oLuOwbQkTmF3fawpoP+RER8U5Ii4mREXIyIKUlflbS2d20C6NZczsZb0uOSDkbEF2csXz7jZXdIOlB/ewDqMpez8TdJ+rSk/bb3Vss+J2mD7TWa/rXgI5I+04P+ANRkLmfjvyfJs5Ser78dAL3CFXRAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkHBH925h9StKPZyxaIunNvjVwZQa1t0HtS6K3TtXZ2wcj4hdnK/Q17O/ZuD0eEWONNVAwqL0Nal8SvXWqX71xGA8kQdiBJJoO+5aGt18yqL0Nal8SvXWqL701+pkdQP80vWcH0CeEHUiikbDbvs32q7YP236giR5asX3E9v5qGurxhnvZanvC9oEZyxbb3mH7UHU/6xx7DfU2ENN4F6YZb/S9a3r6875/Zrc9JOmHkn5P0jFJL0raEBE/6GsjLdg+ImksIhq/AMP2b0p6W9LfRcSvVsv+StLpiHi4+o9yNCL+dEB6e0jS201P413NVrR85jTjkm6X9Edq8L0r9PUH6sP71sSefa2kwxHxWkScl/SUpPUN9DHwImKXpNOXLV4vaVv1eJum/7H0XYveBkJEnIiIPdXjM5LenWa80feu0FdfNBH2FZJen/H8mAZrvveQ9B3bL9ne1HQzs1gWESek6X88kpY23M/l2k7j3U+XTTM+MO9dJ9Ofd6uJsM82ldQgjf/dFBG/LulTkjZXh6uYmzlN490vs0wzPhA6nf68W02E/ZikVTOer5R0vIE+ZhURx6v7CUnPavCmoj757gy61f1Ew/38n0Gaxnu2acY1AO9dk9OfNxH2FyWttn2t7RFJd0na3kAf72F7YXXiRLYXSvqkBm8q6u2SNlaPN0p6rsFeLjEo03i3mmZcDb93jU9/HhF9v0lap+kz8j+S9GdN9NCirw9L+s/q9krTvUl6UtOHdRc0fUR0j6QPSNop6VB1v3iAevu6pP2S9mk6WMsb6u0Tmv5ouE/S3uq2run3rtBXX943LpcFkuAKOiAJwg4kQdiBJAg7kARhB5Ig7EAShB1I4n8BxlTpMLu/V3IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60,   131] loss: 9.177\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[80,   131] loss: 8.555\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100,   131] loss: 8.160\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[120,   131] loss: 8.033\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[140,   131] loss: 7.739\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[160,   131] loss: 7.728\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[180,   131] loss: 7.516\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200,   131] loss: 7.376\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done traininig\n"
     ]
    }
   ],
   "source": [
    "def getRealSamples(data, batch_size):\n",
    "    temp = torch.randperm(batch_size)\n",
    "    result = data[temp, :, :, :]\n",
    "    #rescale to match generator, normall grey scale ranges from [0,255], so to do this rescale we will subtrack by the mid point and divide by the mid point\n",
    "    #(this will force all the values to be between -1 and 1)\n",
    "    result = result.float()\n",
    "    result = (result - 255/2)/(255/2)\n",
    "    return result\n",
    "#define our two optimizers\n",
    "learning_rate = 5e-5\n",
    "#Use two separate RMSprop optimizers, one for the generator and one for the discriminator\n",
    "gOptimizer = torch.optim.RMSprop(gen.parameters(), lr=learning_rate)\n",
    "dOptimizer = torch.optim.RMSprop(dis.parameters(), lr=learning_rate)\n",
    "#Now we will define the training loop:\n",
    "epochs = 200\n",
    "batchesPerEpoch = int(np.floor(train_x.shape[0]/10/batch_size))\n",
    "for epoch in range(epochs):\n",
    "    running_gen_loss = 0\n",
    "    for i in range(batchesPerEpoch):\n",
    "        #number of times we train the critic for each training step on the generator\n",
    "        criticConst = 5\n",
    "        for j in range(criticConst):\n",
    "            #set the gradients of the discriminator to zero\n",
    "            dOptimizer.zero_grad()\n",
    "            #generate a mini batch of gaussian noise\n",
    "            genMiniBatch = torch.rand(batch_size, 100).float()\n",
    "            genMiniBatch = genMiniBatch.cuda()\n",
    "            #get a minibatch of the real data\n",
    "            realMiniBatch = getRealSamples(train_x, batch_size)\n",
    "            realMiniBatch = realMiniBatch.cuda()\n",
    "\n",
    "            #Start by training the discriminator\n",
    "            #Get Dx, the discriminator when applied to the minibatch of real data\n",
    "            Dx = dis(realMiniBatch)\n",
    "            #calculate the loss associated with this step\n",
    "            dxLoss = -1*torch.mean(Dx)\n",
    "            #find the gradients for this step\n",
    "            dxLoss.backward()\n",
    "            #generate fake images\n",
    "            Gz = gen(genMiniBatch.float())\n",
    "            #apply the discriminator to fake images\n",
    "            Gz = Gz.view(batch_size, 1, 28, 28)\n",
    "            Gz = Gz.cuda()\n",
    "            DGz = dis(Gz)\n",
    "            #Find the loss associated with this step\n",
    "            dgzLoss = 1*torch.mean(DGz)\n",
    "            #Accumulate the gradients for this half of the loss\n",
    "            dgzLoss.backward()\n",
    "            #set the total loss as the sum of our two previous losses\n",
    "            dLoss = dgzLoss+dxLoss\n",
    "            dOptimizer.step()\n",
    "            #weight clipping. I used https://discuss.pytorch.org/t/set-constraints-on-parameters-or-layers/23620/7 as a reference\n",
    "            #it appears that j.data will allow us to modify non-leaf nodes of our computational graph despite the fact that they require gradients. \n",
    "            #It also appears that this feature is depreciated, and it shouldn't be used until after our optimizer has taken a step because it might mess \n",
    "            #with the backpropogation. Note: the .clamp_ function does exactly what one would think (it makes sure that all the values of a tensor are \n",
    "            #within the given range) and the _ denotes that the operation is an inplace operation.\n",
    "            for j in dis.parameters():\n",
    "                j.data.clamp_(-.1,.1)\n",
    "        \n",
    "        \n",
    "        #set the generator's gradients to zero\n",
    "        gOptimizer.zero_grad()\n",
    "        genMiniBatch = torch.rand(batch_size*2, 100)\n",
    "        genMiniBatch = genMiniBatch.cuda()\n",
    "        #use the generator to make a new batch of images (even though we have already computed this we need to recompute it for pytorch to understand \n",
    "        #how to backpropogate)\n",
    "        Gz = gen(genMiniBatch.float())\n",
    "        #apply the discriminator to the generated images\n",
    "        Gz = Gz.view(batch_size*2, 1, 28, 28)\n",
    "        Gz = Gz.cuda()\n",
    "        DGz = dis(Gz)\n",
    "        #calculate the loss\n",
    "        gLoss = -1*torch.mean(DGz)\n",
    "        #accumulate the gradients\n",
    "        gLoss.backward()\n",
    "        gOptimizer.step()\n",
    "            \n",
    "        if epoch == 0 and i == 0:\n",
    "            print(\"The generator's first attempt at generating an image: \")\n",
    "            Gz = Gz.view(batch_size*2,1,28,28)\n",
    "            TImShow(Gz[0,:,:,:])\n",
    "        # print statistics and image\n",
    "        running_gen_loss += gLoss.item()\n",
    "        if i == batchesPerEpoch-1 and epoch%20 == 19:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_gen_loss/batchesPerEpoch))\n",
    "            Gz = Gz.view(batch_size*2,1,28,28)\n",
    "            TImShow(Gz[0,:,:,:])\n",
    "            #running_gen_loss = 0.0\n",
    "                \n",
    "print(\"Done traininig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Thoughts on GANs Using the Complete Dataset\n",
    "It doesn't appear to converge to a single solution, and this is probably due to the fact that I am using Wasserstein loss. I could probably get the mode collapse behavior if I used the original paper's loss, or maybe just a more complex dataset, but for now I think that my implementation is somewhat stable for this data set. It does seem to take longer to converge to a consistently good solution, a lot of the things it prints seem to be mixes of multiple digits rather than 1 cohesive digit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of this Progress Report\n",
    "In this progress report I learned how to apply deep learning techniques to 2 more types of data: sequential (text) and tabular. LSTMs presented an interesting solution to sequential models tendency to \"forget\" the beginning of an input sequence through their use of gates which allowed them to curate what information they thought was important throughout the model. This exercise was not only useful for my own learning, but also helped me understand attention in neural networks. I then turned to implementing TabNet which uses ideas from many different domains in machine learning to provide a framework for working with tabular data using deep learning. It did this through an attention mechanism that allowed the neural net to simulate a decision tree through the use of masks. I had originally intended to do more \"exploitation\" of techniques this week, but implementing TabNet turned out to be a much larger undertaking than I had originally thought, involving multiple papers, a new layer type (see ghost batch normalization), and a new activation function (see sparse max), so I ran out of time. I hope next week to apply some of these techniques to data that interests me and pertains to my research outside of class, but for this week I believe that this work is sufficient. Additionally, I quickly addressed feedback with my previous biweekly report by learning how to read and access the CIFAR-10 data set and running my GAN on the complete MNIST data set, which did not exhibit mode collapse due to my use of Wasserstein loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
