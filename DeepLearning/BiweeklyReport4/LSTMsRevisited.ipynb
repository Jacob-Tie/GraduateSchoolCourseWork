{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biweekly Report 4\n",
    "# Jacob Tiede\n",
    "## Revisiting LSTMs/Addressing Feedback from Biweekly Report 2\n",
    "In this notebook I will attempt to solve some of the problems associated with my prior biweekly report. Namely, I will attempt to increase the GPU utilization of my LSTM, and identify what might be causing some of the errors in my model. To start we will copy the code I did from that week:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is a GPU available?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"Is a GPU available?\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id keyword location                                               text  \\\n",
      "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
      "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
      "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
      "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
      "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
      "\n",
      "   target  \n",
      "0       1  \n",
      "1       1  \n",
      "2       1  \n",
      "3       1  \n",
      "4       1  \n"
     ]
    }
   ],
   "source": [
    "#Let's load the data into python\n",
    "trainData = pd.read_csv(\"./Data/Tweet_train.csv\")\n",
    "#To get an idea of what the data contains we will print the first few rows:\n",
    "print(trainData.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the target:\n",
    "trainTarget = trainData['target'].to_numpy()\n",
    "#We'll only use the text to train (no keyword or id)\n",
    "trainText = trainData['text'].to_numpy()\n",
    "from sklearn.model_selection import train_test_split\n",
    "trainText, testText, trainTarget, testTarget = train_test_split(trainText, trainTarget, test_size=0.1, random_state=4)\n",
    "\n",
    "testTarget = torch.from_numpy(testTarget).cuda()\n",
    "trainTarget = torch.from_numpy(trainTarget).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainText.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now need to both separate all of our sentences into words and represent them with a vector, we will do this with a class\n",
    "\n",
    "#This function takes a string and separates it into words\n",
    "def sentToWords(text):\n",
    "    words = []\n",
    "    temp = \"\"\n",
    "    for i in text:\n",
    "        if i != \" \":\n",
    "            temp = temp + i\n",
    "        else:\n",
    "            words.append(temp)\n",
    "            temp = \"\"\n",
    "    words.append(temp)\n",
    "    return words\n",
    "\n",
    "#takes a vector of strings and splits them into words, which then become a vocab dictionary with an index mapping\n",
    "def splitSentences(text):\n",
    "    words = []\n",
    "    temp = \"\"\n",
    "    for para in text:\n",
    "        for i in para:\n",
    "            if i != \" \":\n",
    "                temp = temp + i\n",
    "            else:\n",
    "                words.append(temp)\n",
    "                temp = \"\"\n",
    "        words.append(temp)\n",
    "    mapping = {}\n",
    "    k = 0\n",
    "    for word in words:\n",
    "        if word not in mapping.keys():\n",
    "            mapping[word] = k\n",
    "            k+=1\n",
    "    return mapping\n",
    "vocab = splitSentences(trainText)\n",
    "\n",
    "#converts a sentence to a list of indexes based on a vocab dictionary\n",
    "def sentenceToIdx(sent, mapping):\n",
    "    temp = sentToWords(sent)\n",
    "    mappedSentence = []\n",
    "    for i in temp:\n",
    "        if i in mapping.keys():\n",
    "            mappedSentence.append(mapping[i])\n",
    "        else:\n",
    "            mappedSentence.append(len(mapping.keys()))\n",
    "    return torch.tensor(mappedSentence).view(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making it Faster\n",
    "To do this we can increase the batch size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's get a histogram of sentence length to see if tweets tend to be similarly sized:\n",
    "temp = []\n",
    "for i in range(trainText.shape[0]):\n",
    "    temp.append(len(sentToWords(trainText[i])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(temp, bins = np.unique(np.array(temp)))\n",
    "plt.title(\"Hist of sentence length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to increase GPU usage we will try to find sentences of the same length and group them together, then feed them into the model. As we can see from the above histogram there should be plenty of opportunities to do this, so let's give it a try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (word_embeddings): Embedding(34063, 13)\n",
       "  (lstm): LSTM(13, 6)\n",
       "  (hidden2tag): Linear(in_features=6, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We now need to both separate all of our sentences into words and represent them with a vector, we will do this with a class\n",
    "\n",
    "#This function takes a string and separates it into words\n",
    "def sentToWords(text):\n",
    "    words = []\n",
    "    temp = \"\"\n",
    "    for i in text:\n",
    "        if i != \" \":\n",
    "            temp = temp + i\n",
    "        else:\n",
    "            words.append(temp)\n",
    "            temp = \"\"\n",
    "    words.append(temp)\n",
    "    return words\n",
    "\n",
    "#takes a vector of strings and splits them into words, which then become a vocab dictionary with an index mapping\n",
    "def splitSentences(text):\n",
    "    words = []\n",
    "    temp = \"\"\n",
    "    for para in text:\n",
    "        for i in para:\n",
    "            if i != \" \":\n",
    "                temp = temp + i\n",
    "            else:\n",
    "                words.append(temp)\n",
    "                temp = \"\"\n",
    "        words.append(temp)\n",
    "    mapping = {}\n",
    "    k = 0\n",
    "    for word in words:\n",
    "        if word not in mapping.keys():\n",
    "            mapping[word] = k\n",
    "            k+=1\n",
    "    return mapping\n",
    "vocab = splitSentences(trainText)\n",
    "\n",
    "#converts a sentence to a list of indexes based on a vocab dictionary\n",
    "def sentenceToIdx(sent, mapping):\n",
    "    temp = sentToWords(sent)\n",
    "    mappedSentence = []\n",
    "    for i in temp:\n",
    "        if i in mapping.keys():\n",
    "            mappedSentence.append(mapping[i])\n",
    "        else:\n",
    "            mappedSentence.append(len(mapping.keys()))\n",
    "    return torch.tensor(mappedSentence).view(1,-1)\n",
    "\n",
    "#create the LSTM Classifier, see https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.tagset_size = tagset_size\n",
    "        #the dimension of the hidden state of our LSTM\n",
    "        self.hidden_dim = hidden_dim\n",
    "        #create the trainable word embedding\n",
    "        self.word_embeddings = nn.Embedding(vocab_size+1, embedding_dim)\n",
    "        \n",
    "        #we only have a single LSTM layer in this model\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        #from the pytorch tutorial, they were trying to tag words with things like \"noun\" or \"verb\", but I will only use the final output, so a better name might\n",
    "        #be hidden to prediction, but for the sake of following the notation of the tutorial I will keep the naming\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        #embed the word vectors into a smaller dimension space\n",
    "        embeds = self.word_embeddings(sentence).view(sentence.shape[1], self.batch_size, -1)\n",
    "        #send this through our LSTM layer\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        #tag space is the \"tags\" of each word, but remember that we only actually need to apply this neural net to the last \"word\" in the sentence (the final output of the LSTM)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(self.batch_size,sentence.shape[1], -1)[:,sentence.shape[1]-1,:])\n",
    "        out = torch.sigmoid(tag_space)\n",
    "        return out\n",
    "    def setBatchSize(self, x):\n",
    "        self.batch_size = x\n",
    "model = LSTMClassifier(int(np.floor(np.power(len(vocab.keys()), 1/4))), 6, int(len(vocab.keys())), 1)\n",
    "model.cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10,  6851] loss: 0.013\n",
      "Test set accuracy for this epoch: 0.4816272965879265\n",
      "[20,  6851] loss: 0.012\n",
      "Test set accuracy for this epoch: 0.5485564304461942\n",
      "[30,  6851] loss: 0.012\n",
      "Test set accuracy for this epoch: 0.5643044619422573\n",
      "[40,  6851] loss: 0.012\n",
      "Test set accuracy for this epoch: 0.5643044619422573\n",
      "[50,  6851] loss: 0.012\n",
      "Test set accuracy for this epoch: 0.5656167979002624\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "model = LSTMClassifier(int(np.floor(np.power(len(vocab.keys()), 1/4))), 6, int(len(vocab.keys())), 1)\n",
    "model.cuda()\n",
    "#train the model similarly to how we have before\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCELoss()\n",
    "epochs = 50\n",
    "#Batch size will not always be 64, this is more like max batch size\n",
    "batch_size = 64\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    temp = torch.randperm(trainText.shape[0])\n",
    "    trainTarget = trainTarget[temp]\n",
    "    trainText = trainText[temp]\n",
    "    \n",
    "    moreData = True\n",
    "    j = 0\n",
    "    \n",
    "    #keep a list of unprocessed indicies\n",
    "    validInd = list(range(trainText.shape[0]))\n",
    "    while moreData:\n",
    "        if validInd[j] != -1:\n",
    "            optimizer.zero_grad()\n",
    "            words = len(sentToWords(trainText[j]))\n",
    "            #look for sentences that we have not already process\n",
    "            i = j+1\n",
    "            #we know that we will need to include j\n",
    "            temp = [sentenceToIdx(trainText[j], vocab).cuda()]\n",
    "            targetInd = [j]\n",
    "            #look through the whole list of sentences for similarly sized sentences to j, if we get batch_size in temp then stop, or if we have iterated through\n",
    "            #the whole list\n",
    "            while len(temp) < batch_size and i < trainText.shape[0]:\n",
    "                #if we have an unprocessed index\n",
    "                if validInd[i] != -1:\n",
    "                    tempWords = len(sentToWords(trainText[i]))\n",
    "                    if tempWords == words:\n",
    "                        temp.append(sentenceToIdx(trainText[i], vocab).cuda())\n",
    "                        validInd[i] = -1\n",
    "                        targetInd.append(i)\n",
    "                i+=1\n",
    "            #temp was a list of tensors so we can just cat those together, then the rest is the same as in my previous biweekly report\n",
    "            input = torch.cat(temp, dim = 0)\n",
    "            model.setBatchSize(input.shape[0])\n",
    "            output = model(input)\n",
    "            temp = trainTarget[targetInd]\n",
    "            loss = criterion(output.float(), temp.float().view(temp.shape[0],1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "        j+=1\n",
    "        if j == trainText.shape[0] -1 and epoch%10 == 9:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, j + 1, running_loss / trainText.shape[0]))\n",
    "            acc = 0\n",
    "            model.setBatchSize(1)\n",
    "            for k in range(int(testText.shape[0])):\n",
    "                input = sentenceToIdx(testText[k], vocab).cuda()\n",
    "                output = model(input)\n",
    "                if output >= .5:\n",
    "                    output = 1\n",
    "                else:\n",
    "                    output = 0\n",
    "                if output == testTarget[k]:\n",
    "                    acc += 1\n",
    "            print(\"Test set accuracy for this epoch: \" + str(acc/testText.shape[0]))\n",
    "        if j == trainText.shape[0]-1:\n",
    "            moreData = False\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './lstmModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (word_embeddings): Embedding(34063, 13)\n",
       "  (lstm): LSTM(13, 6)\n",
       "  (hidden2tag): Linear(in_features=6, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTMClassifier(int(np.floor(np.power(len(vocab.keys()), 1/4))), 6, int(len(vocab.keys())), 1)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (word_embeddings): Embedding(34063, 13)\n",
       "  (lstm): LSTM(13, 6)\n",
       "  (hidden2tag): Linear(in_features=6, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTMClassifier(int(np.floor(np.power(len(vocab.keys()), 1/4))), 6, int(len(vocab.keys())), 1)\n",
    "model.load_state_dict(torch.load('./lstmModel'))\n",
    "model.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy for this epoch: 0.5656167979002624\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "model.setBatchSize(1)\n",
    "for k in range(int(testText.shape[0])):\n",
    "    input = sentenceToIdx(testText[k], vocab).cuda()\n",
    "    output = model(input)\n",
    "    if output >= .5:\n",
    "        output = 1\n",
    "    else:\n",
    "        output = 0\n",
    "    if output == testTarget[k]:\n",
    "        acc += 1\n",
    "print(\"Test set accuracy for this epoch: \" + str(acc/testText.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see this is much faster, but the test set accuracy has suffered. It is pertinent to first ask if we have implement this batch process correctly, so to this end I will set batch size to 1 and then to 2, to see if we reporduce my results from the seecond biweekly report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10,  6851] loss: 0.561\n",
      "Test set accuracy for this epoch: 0.60498687664042\n",
      "[20,  6851] loss: 0.404\n",
      "Test set accuracy for this epoch: 0.7060367454068242\n",
      "[30,  6851] loss: 0.283\n",
      "Test set accuracy for this epoch: 0.7165354330708661\n",
      "[40,  6851] loss: 0.193\n",
      "Test set accuracy for this epoch: 0.7401574803149606\n",
      "[50,  6851] loss: 0.128\n",
      "Test set accuracy for this epoch: 0.7244094488188977\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "model = LSTMClassifier(int(np.floor(np.power(len(vocab.keys()), 1/4))), 6, int(len(vocab.keys())), 1)\n",
    "model.cuda()\n",
    "#train the model similarly to how we have before\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCELoss()\n",
    "epochs = 50\n",
    "batch_size = 1\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    temp = torch.randperm(trainText.shape[0])\n",
    "    trainTarget = trainTarget[temp]\n",
    "    trainText = trainText[temp]\n",
    "    \n",
    "    moreData = True\n",
    "    j = 0\n",
    "    \n",
    "    validInd = list(range(trainText.shape[0]))\n",
    "    while moreData:\n",
    "        if validInd[j] != -1:\n",
    "            optimizer.zero_grad()\n",
    "            words = len(sentToWords(trainText[j]))\n",
    "            i = j+1\n",
    "            temp = [sentenceToIdx(trainText[j], vocab).cuda()]\n",
    "            targetInd = [j]\n",
    "            while len(temp) < batch_size and i < trainText.shape[0]:\n",
    "                if validInd[i] != 0:\n",
    "                    tempWords = len(sentToWords(trainText[i]))\n",
    "                    if tempWords == words:\n",
    "                        temp.append(sentenceToIdx(trainText[i], vocab).cuda())\n",
    "                        validInd[i] = -1\n",
    "                        targetInd.append(i)\n",
    "                i+=1  \n",
    "            input = torch.cat(temp, dim = 0)\n",
    "            model.setBatchSize(input.shape[0])\n",
    "            output = model(input)\n",
    "            temp = trainTarget[targetInd]\n",
    "            loss = criterion(output.float(), temp.float().view(temp.shape[0],1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "        j+=1\n",
    "        if j == trainText.shape[0] -1 and epoch%10 == 9:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, j + 1, running_loss / trainText.shape[0]))\n",
    "            acc = 0\n",
    "            model.setBatchSize(1)\n",
    "            for k in range(int(testText.shape[0])):\n",
    "                input = sentenceToIdx(testText[k], vocab).cuda()\n",
    "                output = model(input)\n",
    "                if output >= .5:\n",
    "                    output = 1\n",
    "                else:\n",
    "                    output = 0\n",
    "                if output == testTarget[k]:\n",
    "                    acc += 1\n",
    "            print(\"Test set accuracy for this epoch: \" + str(acc/testText.shape[0]))\n",
    "        if j == trainText.shape[0]-1:\n",
    "            moreData = False\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10,  6851] loss: 0.322\n",
      "Test set accuracy for this epoch: 0.5879265091863517\n",
      "[20,  6851] loss: 0.275\n",
      "Test set accuracy for this epoch: 0.6535433070866141\n",
      "[30,  6851] loss: 0.228\n",
      "Test set accuracy for this epoch: 0.6679790026246719\n",
      "[40,  6851] loss: 0.185\n",
      "Test set accuracy for this epoch: 0.6771653543307087\n",
      "[50,  6851] loss: 0.148\n",
      "Test set accuracy for this epoch: 0.6811023622047244\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "model = LSTMClassifier(int(np.floor(np.power(len(vocab.keys()), 1/4))), 6, int(len(vocab.keys())), 1)\n",
    "model.cuda()\n",
    "#train the model similarly to how we have before\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCELoss()\n",
    "epochs = 50\n",
    "batch_size = 2\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    temp = torch.randperm(trainText.shape[0])\n",
    "    trainTarget = trainTarget[temp]\n",
    "    trainText = trainText[temp]\n",
    "    \n",
    "    moreData = True\n",
    "    j = 0\n",
    "    \n",
    "    validInd = list(range(trainText.shape[0]))\n",
    "    while moreData:\n",
    "        if validInd[j] != -1:\n",
    "            optimizer.zero_grad()\n",
    "            words = len(sentToWords(trainText[j]))\n",
    "            i = j+1\n",
    "            temp = [sentenceToIdx(trainText[j], vocab).cuda()]\n",
    "            targetInd = [j]\n",
    "            while len(temp) < batch_size and i < trainText.shape[0]:\n",
    "                if validInd[i] != 0:\n",
    "                    tempWords = len(sentToWords(trainText[i]))\n",
    "                    if tempWords == words:\n",
    "                        temp.append(sentenceToIdx(trainText[i], vocab).cuda())\n",
    "                        validInd[i] = -1\n",
    "                        targetInd.append(i)\n",
    "                i+=1  \n",
    "            input = torch.cat(temp, dim = 0)\n",
    "            model.setBatchSize(input.shape[0])\n",
    "            output = model(input)\n",
    "            temp = trainTarget[targetInd]\n",
    "            loss = criterion(output.float(), temp.float().view(temp.shape[0],1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "        j+=1\n",
    "        if j == trainText.shape[0] -1 and epoch%10 == 9:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, j + 1, running_loss / trainText.shape[0]))\n",
    "            acc = 0\n",
    "            model.setBatchSize(1)\n",
    "            for k in range(int(testText.shape[0])):\n",
    "                input = sentenceToIdx(testText[k], vocab).cuda()\n",
    "                output = model(input)\n",
    "                if output >= .5:\n",
    "                    output = 1\n",
    "                else:\n",
    "                    output = 0\n",
    "                if output == testTarget[k]:\n",
    "                    acc += 1\n",
    "            print(\"Test set accuracy for this epoch: \" + str(acc/testText.shape[0]))\n",
    "        if j == trainText.shape[0]-1:\n",
    "            moreData = False\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to reproduce the accuracy from the first biweekly reports in both these cases (or close to the same accuracy). There are several potential causes for this: we could be observing the so called \"generalizability gap\" that I discussed in my second biweekly report (see the ghost batch norm section of that document), but I think that is somewhat less likely given how low the loss is for the batch size = 64 case. What I think is more likely is that we are overfitting to that data. This is probably because we are feeding the model sentences of the same length in batches. Sentences of similar length are probably related to each other to an extent, so we are probably encouraging the model to overfit to this knowledge. I tried to solve this through sampling with replacement (rather than forcing the model to look at each sample once per epoch), but this made the overfitting worse. I'm not quite sure how to solve this since I think it is intrinsic to the fact that I'm using a larger batch size. I'm going to close this issue for now since my implementation does speed up training, but some sort of regularization is in order to keep the model from overfitting.\n",
    "### Looking at the Mistakes the Model Makes:\n",
    "First we'll need to retrain the model using batch size = 1 to get similar accuracy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10,  6851] loss: 0.557\n",
      "Test set accuracy for this epoch: 0.6797900262467191\n",
      "[20,  6851] loss: 0.401\n",
      "Test set accuracy for this epoch: 0.7230971128608924\n",
      "[30,  6851] loss: 0.289\n",
      "Test set accuracy for this epoch: 0.7270341207349081\n",
      "[40,  6851] loss: 0.199\n",
      "Test set accuracy for this epoch: 0.7335958005249343\n",
      "[50,  6851] loss: 0.133\n",
      "Test set accuracy for this epoch: 0.7283464566929134\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "model = LSTMClassifier(int(np.floor(np.power(len(vocab.keys()), 1/4))), 6, int(len(vocab.keys())), 1)\n",
    "model.cuda()\n",
    "#train the model similarly to how we have before\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCELoss()\n",
    "epochs = 50\n",
    "batch_size = 1\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    temp = torch.randperm(trainText.shape[0])\n",
    "    trainTarget = trainTarget[temp]\n",
    "    trainText = trainText[temp]\n",
    "    \n",
    "    moreData = True\n",
    "    j = 0\n",
    "    \n",
    "    validInd = list(range(trainText.shape[0]))\n",
    "    while moreData:\n",
    "        if validInd[j] != -1:\n",
    "            optimizer.zero_grad()\n",
    "            words = len(sentToWords(trainText[j]))\n",
    "            i = j+1\n",
    "            temp = [sentenceToIdx(trainText[j], vocab).cuda()]\n",
    "            targetInd = [j]\n",
    "            while len(temp) < batch_size and i < trainText.shape[0]:\n",
    "                if validInd[i] != 0:\n",
    "                    tempWords = len(sentToWords(trainText[i]))\n",
    "                    if tempWords == words:\n",
    "                        temp.append(sentenceToIdx(trainText[i], vocab).cuda())\n",
    "                        validInd[i] = -1\n",
    "                        targetInd.append(i)\n",
    "                i+=1  \n",
    "            input = torch.cat(temp, dim = 0)\n",
    "            model.setBatchSize(input.shape[0])\n",
    "            output = model(input)\n",
    "            temp = trainTarget[targetInd]\n",
    "            loss = criterion(output.float(), temp.float().view(temp.shape[0],1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "        j+=1\n",
    "        if j == trainText.shape[0] -1 and epoch%10 == 9:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, j + 1, running_loss / trainText.shape[0]))\n",
    "            acc = 0\n",
    "            model.setBatchSize(1)\n",
    "            for k in range(int(testText.shape[0])):\n",
    "                input = sentenceToIdx(testText[k], vocab).cuda()\n",
    "                output = model(input)\n",
    "                if output >= .5:\n",
    "                    output = 1\n",
    "                else:\n",
    "                    output = 0\n",
    "                if output == testTarget[k]:\n",
    "                    acc += 1\n",
    "            print(\"Test set accuracy for this epoch: \" + str(acc/testText.shape[0]))\n",
    "        if j == trainText.shape[0]-1:\n",
    "            moreData = False\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified Sentence:\n",
      "#Cowboys: George: Injury woes took Claiborne from first round to trying to stick around; can he do it?:  http://t.co/12giQbVLYs\n",
      "Classified as: 1\n",
      "Misclassified Sentence:\n",
      "Bridge collapse not natural calamity but man-made: MPP lambasts Congress - KanglaOnline http://t.co/jp9XylA3C5  #Yugvani\n",
      "Classified as: 0\n",
      "Misclassified Sentence:\n",
      "hurricane?? sick!\n",
      "Classified as: 0\n",
      "Misclassified Sentence:\n",
      "Beautiful lightning as seen from plane window http://t.co/5CwUyLnFUm http://t.co/1tyYqFz13D\n",
      "Classified as: 1\n",
      "Misclassified Sentence:\n",
      "Reddit Will Now Quarantine Offensive Content http://t.co/LTmgdP6Jaf\n",
      "Classified as: 0\n",
      "Misclassified Sentence:\n",
      "On the 2nd year the officer team running the club collapsed. A few influential members betrayed everyone's trust severing the community.\n",
      "Classified as: 1\n",
      "Misclassified Sentence:\n",
      "ÛÏYou see the devastation &amp; itÛªs shocking.ÛÏ  Firefighters continue to battle flames in California. benstracy reports Û_\n",
      "Classified as: 0\n",
      "Misclassified Sentence:\n",
      "@NEPD_Loyko Texans hope you are wrong. Radio in Houston have him as starter after Foster injury\n",
      "Classified as: 1\n",
      "Misclassified Sentence:\n",
      "Meltdown\n",
      "Classified as: 1\n",
      "Misclassified Sentence:\n",
      "@cllrraymogford Indeed Ray devastation would be far more comprehensive #Hiroshima\n",
      "Classified as: 0\n"
     ]
    }
   ],
   "source": [
    "# let's find the sentences that it misclassifies:\n",
    "l = 0\n",
    "numSentencesToPrint = 10\n",
    "for k in range(int(testText.shape[0])):\n",
    "    input = sentenceToIdx(testText[k], vocab).cuda()\n",
    "    output = model(input)\n",
    "    if output >= .5:\n",
    "        output = 1\n",
    "    else:\n",
    "        output = 0\n",
    "    if output != testTarget[k] and l < numSentencesToPrint:\n",
    "        print(\"Misclassified Sentence:\")\n",
    "        print(testText[k])\n",
    "        print(\"Classified as: \" + str(output))\n",
    "        l+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like one problem is the brevity of the tweet, shorter tweets (like the word \"Meltdown\" with no other context) are hard to classify (for obvious reasons). Interestingly, it looks like most of these misclassifications are reasonable, but one interesting tweet is \"ÛÏYou see the devastation &amp; itÛªs shocking.ÛÏ  Firefighters continue to battle flames in California. benstracy reports Û_\" which is a disaster tweet (obviously if you ignore the strange characters), but the model still misclassified it. This is interesting because it looks like the low occurance of tweets like this (with strange characters) might have confused the model since these are probably uncommon in general. This means that it sees that as unlike anything it has seen before, so it probably defaults to a classification of 0 since it can't tell that there is a disaster. I'm not quite sure how this problem is typically solved, but word embeddings (which we've already implemented) seem to actually do an okay job at this correction (or rather ignoring misspelled words all together), but it would be better to never have the misspelling in the first place. One method of solving this is detailed by https://towardsdatascience.com/correcting-your-spelling-error-with-4-operations-50bcfd519bb8. This method is basically just doing simple operations on a given word (like adding or deleting a character) and then seeing which candidate word created by this process has the highest likelihood of being within some corpus of words. An implementation of this can be found here: https://pypi.org/project/pyspellchecker/, let's first replicate some of the documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happening\n",
      "{'henning', 'happening', 'penning'}\n"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "#This example from the documentation at: https://pypi.org/project/pyspellchecker/\n",
    "spell = SpellChecker()\n",
    "\n",
    "# find those words that may be misspelled\n",
    "misspelled = spell.unknown(['something', 'is', 'hapenning', 'here'])\n",
    "\n",
    "for word in misspelled:\n",
    "    # Get the one `most likely` answer\n",
    "    print(spell.correction(word))\n",
    "\n",
    "    # Get a list of `likely` options\n",
    "    print(spell.candidates(word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it is working, let's see if running our dataset through this spell checker will help our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ÛÏYou see the devastation camp itÛªs shocking.ÛÏ a Firefighters continue to battle flames in California. benstracy reports Û_ '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fixSentence(sentence):\n",
    "    words = sentToWords(sentence)\n",
    "    misspelled = spell.unknown(words)\n",
    "    for i in range(len(words)):\n",
    "        for j in misspelled:\n",
    "            if words[i] == j:\n",
    "                words[i] = spell.correction(words[i])\n",
    "    string = \"\"\n",
    "    for i in words:\n",
    "        string += i + \" \"\n",
    "    return string\n",
    "test = \"ÛÏYou see the devastation &amp; itÛªs shocking.ÛÏ  Firefighters continue to battle flames in California. benstracy reports Û_\"\n",
    "fixSentence(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not great, but it looks better than before, so let's run the whole data set through:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (6850 of 6850) |####################| Elapsed Time: 0:56:21 Time:  0:56:21\n",
      "100% (761 of 761) |######################| Elapsed Time: 0:06:20 Time:  0:06:20\n"
     ]
    }
   ],
   "source": [
    "import progressbar\n",
    "with progressbar.ProgressBar(max_value=trainText.shape[0]-1) as bar:\n",
    "    for i in range(trainText.shape[0]):\n",
    "        trainText[i] = fixSentence(trainText[i])\n",
    "        bar.update(i)\n",
    "with progressbar.ProgressBar(max_value=testText.shape[0]-1) as bar:\n",
    "    for i in range(testText.shape[0]):\n",
    "        testText[i] = fixSentence(testText[i])\n",
    "        bar.update(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can attempt to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10,  6851] loss: 0.608\n",
      "Test set accuracy for this epoch: 0.6509186351706037\n",
      "[20,  6851] loss: 0.440\n",
      "Test set accuracy for this epoch: 0.6942257217847769\n",
      "[30,  6851] loss: 0.312\n",
      "Test set accuracy for this epoch: 0.7086614173228346\n",
      "[40,  6851] loss: 0.211\n",
      "Test set accuracy for this epoch: 0.6942257217847769\n",
      "[50,  6851] loss: 0.139\n",
      "Test set accuracy for this epoch: 0.6902887139107612\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "vocab = splitSentences(trainText)\n",
    "model = LSTMClassifier(int(np.floor(np.power(len(vocab.keys()), 1/4))), 6, int(len(vocab.keys())), 1)\n",
    "model.cuda()\n",
    "#train the model similarly to how we have before\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCELoss()\n",
    "epochs = 50\n",
    "batch_size = 1\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    temp = torch.randperm(trainText.shape[0])\n",
    "    trainTarget = trainTarget[temp]\n",
    "    trainText = trainText[temp]\n",
    "    \n",
    "    moreData = True\n",
    "    j = 0\n",
    "    \n",
    "    validInd = list(range(trainText.shape[0]))\n",
    "    while moreData:\n",
    "        if validInd[j] != -1:\n",
    "            optimizer.zero_grad()\n",
    "            words = len(sentToWords(trainText[j]))\n",
    "            i = j+1\n",
    "            temp = [sentenceToIdx(trainText[j], vocab).cuda()]\n",
    "            targetInd = [j]\n",
    "            while len(temp) < batch_size and i < trainText.shape[0]:\n",
    "                if validInd[i] != 0:\n",
    "                    tempWords = len(sentToWords(trainText[i]))\n",
    "                    if tempWords == words:\n",
    "                        temp.append(sentenceToIdx(trainText[i], vocab).cuda())\n",
    "                        validInd[i] = -1\n",
    "                        targetInd.append(i)\n",
    "                i+=1  \n",
    "            input = torch.cat(temp, dim = 0)\n",
    "            model.setBatchSize(input.shape[0])\n",
    "            output = model(input)\n",
    "            temp = trainTarget[targetInd]\n",
    "            loss = criterion(output.float(), temp.float().view(temp.shape[0],1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "        j+=1\n",
    "        if j == trainText.shape[0] -1 and epoch%10 == 9:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, j + 1, running_loss / trainText.shape[0]))\n",
    "            acc = 0\n",
    "            model.setBatchSize(1)\n",
    "            for k in range(int(testText.shape[0])):\n",
    "                input = sentenceToIdx(testText[k], vocab).cuda()\n",
    "                output = model(input)\n",
    "                if output >= .5:\n",
    "                    output = 1\n",
    "                else:\n",
    "                    output = 0\n",
    "                if output == testTarget[k]:\n",
    "                    acc += 1\n",
    "            print(\"Test set accuracy for this epoch: \" + str(acc/testText.shape[0]))\n",
    "        if j == trainText.shape[0]-1:\n",
    "            moreData = False\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like spell checking really didn't make that much of a difference, probably due to the fact that we are using word embeddings already. This leads me to believe that the lower accuracy is more likely due to the way that our neural network is structured rather than the data itself. Even though LSTMs have made leaps and bounds in terms of remembering the whole sentence, it still is not an ideal model. I've heard that some more modern models like transformers are better at this, but for now I'm going to leave this analysis here since I believe I have addressed the feedback given to me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
