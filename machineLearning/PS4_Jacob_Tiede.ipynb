{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Competition Homework Set  \n",
    "## By Jacob Tiede  \n",
    "### Exploritory Data Analysis:  \n",
    "**Note: all of this analysis will submitted in a separate file since I used R to do the EDA. This will just be a summary of the findings**  \n",
    "\n",
    "Looking through the data the things that jumped out to me were class imbalances. One can see that males are much more represented in the data than females, people who earn less than 50,000$ a year are much more prevalent than those that earn more, and people who identified as white were more prevalent than any other race. After trying multiple techniques (oversampling to make the response variable balanced, and undersampling, as well as doing bagged models where the models were trained on balanced data) I've found that the only class imbalance that helps with predictive strenth is accounting for the sex of participants. I've also decided to take the log of age and hours per week (despite the fact that my methods should be somewhat resilient to ourliers), because this seems to help the accuracy a little. Other than that the EDA showed some possible feature crosses to try, and the fact that there are many outliers (also, I did observe these class imbalances in the testing set). \n",
    "\n",
    "### Feature Engineering: \n",
    "I decided to include three feature crosses between education and gender, education and race, and gender and occupation. After trying many crosses these ended up being the two that helped predictions the most though more were possible (and the exploritory data analysis may have suggested others, but these were the only ones that helped). I opted to drop the education column since it seemed to be redundant to include it with the education number (as indicated by the EDA). I've also decided to include a 'net capital' column  which will just be the capital gain - capital loss (since this is probably more important than either number in isolation), and a weighted buying power column (hours per week multiplied by education number) since people who work more with less education might still earn more than people with high education who work very few hours, so I thought I would tell the model this explicitly.\n",
    "\n",
    "### Methods:  \n",
    "I opted for a tree-based model because, after doing some research, I found that neural networks/deep learning are only effective for a very large amount of data (way more than what we were given), and I wanted to do an ensemble method that would converge somewhat quickly, so trees seemed like a natural choice. Also there were many outliers in the data and I believe that tree methods (if tuned correctly) are more robust to this problem than other methods. To solve the class imbalance problems I created a method that would randomly generate samples that contained an even number of male and female participants, I then trained both an XGBoosted forest and a Catboosted forest for each sample (each of the libraries use different methods of splitting the data so using both is better than using either separately). I then ensemble these together by finding each one's accuracy on a validation set, and I weight each one according to $weight = \\frac{1}{1-accuracy}$. I chose this weight function since it exaggerates small differences in accuracy of each model (since all of them have accuracies that are close to one another), but it also gives all models some relatively high base-line importance (unlike a function like $-log(1-x)$). The prediction is then:\n",
    "$$\n",
    "\\sum_{i=1}^{N}(weight(i)*prediction(i))\n",
    "$$\n",
    "Where prediction(i) = 1 if the ith model predicts 1 and prediction(i) = -1 if the ith model predicts 0. Then, if that sum is > 0 then our model would predict a class of 1, and if it is < 0 we would predict a 0. Then, as some side notes I used a label encoder rather than one hot encoding becuase label encoding generates no new features so it is much faster in general (especially when I am making 150 models to ensemble), and I opted to use yeo-johnson normalization with scaling (which this particular scalar does by default unless you tell it not to), becuase it got the best results of all the scalar/normalization combinations that I tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set:\n",
      "0.8854774332207553\n"
     ]
    }
   ],
   "source": [
    "#Per the Syllabus see citations for all these packages at the end of the document\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import catboost as cat\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\"\"\"\n",
    "Description: Due to over representation of people earning less than 50K per year and men in the data I have to\n",
    "make this method to generate N sample with numPerSample data points that enforce one equal numbers of these \n",
    "two conditions\n",
    "Note: I currently do not use the equal representation of 50K per year, but I left it in since I might use it for\n",
    "a later iteration\n",
    "\n",
    "@param N: Number of samples to take with balanced classes\n",
    "@param numPerSample: number of data points in each sample\n",
    "@param trainData: The data that we will use to generate balanced classes of men and women\n",
    "@param trainResponse: The data we will use to generate balanced classes of <50K and >50k earnings\n",
    "@param genderindex: tells us which column of trainData contains information on gender\n",
    "\n",
    "Return: an output matrix where we have a repeating pattern every 4 rows. The first row will be a sample of female\n",
    "indicies, the second will be male indicies, the third will be a list of indicies where someone earned >=50k and the\n",
    "fourth will be people will earn <=50k. This will generate a matrix that is 4N by numPerSample in size\n",
    "\"\"\"\n",
    "def baggingIndices(N, numPerSample, trainData, trainResponse, genderindex):\n",
    "    #initialize our return matrix\n",
    "    output = []\n",
    "    #initialize a list that will contain all indicies that have a female in them (though this is arbitrary)\n",
    "    listOfFemaleInd = []\n",
    "    #Initialize a list of row indicies that have a male participant\n",
    "    listOfMaleInd = []\n",
    "    #extract the gender column from the data\n",
    "    gender = trainData[:,genderIndex]\n",
    "    #since we are assuming that the data has already been normalized we will set our discriminant as the first value\n",
    "    #of gender (this is why calling the lists female and male are arbitrary since the first value could be either)\n",
    "    disc = gender[0]\n",
    "    for i in range(0, len(gender)):\n",
    "        #If row i has our discriminant gender assume that it is female (again it doesn't matter since we are generating\n",
    "        #equal numbers of both)\n",
    "        if gender[i] == disc:\n",
    "            listOfFemaleInd.append(i)\n",
    "        #Else assume it they are male\n",
    "        else:\n",
    "            listOfMaleInd.append(i)\n",
    "    #Do a similar process for people earning >=50k vs <=50k\n",
    "    listOfPosOutcomes = []\n",
    "    listOfNegOutcomes = []\n",
    "    for i in range(0, trainResponse.shape[0]):\n",
    "        if trainResponse[i] == 1:\n",
    "            listOfPosOutcomes.append(i)\n",
    "        else:\n",
    "            listOfNegOutcomes.append(i)\n",
    "    #Set seed for reproducability\n",
    "    np.random.seed(0)\n",
    "    #generate N samples that will be put in our output matrix\n",
    "    for i in range(0, N):\n",
    "        #if i==0 we need to use a stack method to generate the matrix because of the way numpy arrays work\n",
    "        \n",
    "        #Essentially for each sample we will generate a random sample of indicies from our list of indices for\n",
    "        #each class that we want to balance. We will get numPerSample indicies from each of our lists without\n",
    "        #replacement, so we will have ensured that our end list of indicies have the same number of either gender\n",
    "        #or >=50k <=50k. These will then be stacked in the output matrix\n",
    "        if i > 0:\n",
    "            femaleInd = np.random.choice(listOfFemaleInd, numPerSample, replace = False)\n",
    "            maleInd = np.random.choice(listOfMaleInd, numPerSample, replace = False)\n",
    "            posInd = np.random.choice(listOfPosOutcomes, numPerSample, replace = False)\n",
    "            negInd = np.random.choice(listOfNegOutcomes, numPerSample, replace = False)\n",
    "\n",
    "            output = np.append(output, [femaleInd], axis = 0)\n",
    "            output = np.append(output, [maleInd], axis = 0)\n",
    "            output = np.append(output, [posInd], axis = 0)\n",
    "            output = np.append(output, [negInd], axis = 0)\n",
    "        else:\n",
    "            femaleInd = np.random.choice(listOfFemaleInd, numPerSample, replace = False)\n",
    "            maleInd = np.random.choice(listOfMaleInd, numPerSample, replace = False)\n",
    "            posInd = np.random.choice(listOfPosOutcomes, numPerSample, replace = False)\n",
    "            negInd = np.random.choice(listOfNegOutcomes, numPerSample, replace = False)\n",
    "\n",
    "            output = femaleInd\n",
    "            output = np.stack((output, maleInd))\n",
    "            output = np.append(output, [posInd], axis = 0)\n",
    "            output = np.append(output, [negInd], axis = 0)\n",
    "    return output\n",
    "\n",
    "\"\"\"\n",
    "Description: Given a list of XGboosted, and Catboosted models each with a different accuracy on a validation set\n",
    "this method will ensemble them to get one set of predictions. It does this by predicting what each model says on\n",
    "the testData and weights that prediction according to 1/(1-accuracy of model i). \n",
    "\n",
    "@param baggedModelsXG: the XGBoosted models that were trained on data with balanced classes\n",
    "@param confidXG: accuracy of each XGBoosted model on a validation set\n",
    "@param baggedModelsCat: the catBoosted models that were trained on data with balanced classes\n",
    "@param confidCat: accuracy of each catBoosted model on a validation set\n",
    "\n",
    "return: a vector of what we classifed each entry in the test data\n",
    "\"\"\"\n",
    "def baggedPredict(baggedModelsXG, confidXG, baggedModelsCat, confidCat, testData):\n",
    "    #make a dmatrix of the testData so we can call predict\n",
    "    Dmatrix_test = xgb.DMatrix(testData, label=None)\n",
    "    #Initialize a consensus matrix (this will keep track of what each model says about each data point)\n",
    "    #each row will represent a model and the ith column will represent the jth model's prediction on the ith data\n",
    "    #point (jth row)\n",
    "    consensus = np.zeros((len(baggedModelsXG) + len(baggedModelsCat)+1, testData.shape[0]))\n",
    "    #Iterate through each XG boosted model\n",
    "    for i in range(0,len(baggedModelsXG)):\n",
    "        #generate predictions\n",
    "        predictions = baggedModelsXG[i].predict(Dmatrix_test)\n",
    "        #Get the best prediction in each line\n",
    "        mostconfidXGPred = np.zeros(predictions.shape[0])\n",
    "        for j in range(0, predictions.shape[0]):\n",
    "            if predictions[j] > .5:\n",
    "                mostconfidXGPred[j] = 1\n",
    "            else:\n",
    "                #set this to -1 so that we don't have a bias toward positive cases\n",
    "                mostconfidXGPred[j] = -1\n",
    "            consensus[i,j] = 1/(1-confidXG[i])*mostconfidXGPred[j]\n",
    "    #Iterate through each cat boosted model\n",
    "    for i in range(len(baggedModelsXG), len(baggedModelsXG) + len(baggedModelsCat)):\n",
    "        predictions = baggedModelsCat[i-len(baggedModelsXG)]. predict(testData)\n",
    "        for j in range(0, len(predictions)):\n",
    "            #remove bias toward positive cases\n",
    "            if predictions[j] == 0:\n",
    "                predictions[j] = -1\n",
    "            consensus[i,j] = 1/(1-confidCat[i-len(baggedModelsXG)])*predictions[j]\n",
    "    #each rowsum will represent a single prediction\n",
    "    consensus = np.sum(consensus, axis = 0)\n",
    "    #turn predictions either into a 0 or a 1\n",
    "    for i in range(0, len(consensus)):\n",
    "        if consensus[i] > 0:\n",
    "            consensus[i] = 1\n",
    "        else:\n",
    "            consensus[i] = 0\n",
    "    return consensus\n",
    "\n",
    "#BEGIN PREPROCESSING\n",
    "#TRAIN DATA LOADING, FEATURE CROSSES, NEW FEATURE CREATION, AND TRANSFORMS\n",
    "#load the training data in with names given to each column\n",
    "trainData = pd.read_csv(\"./Pr4Data/train-features.csv\", header=None,\n",
    "                       names = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\", \n",
    "                                \"marital_status\", \"occupation\", \"relationship\",\n",
    "                                \"race\", \"sex\", \"capital_gain\", \"capital_loss\", \"hoursPerWeek\", \"nativeCountry\"])\n",
    "#create new features\n",
    "#NOTE: lambda is a make shift function that you define temporarily. So what this line of code means is:\n",
    "#apply the function lambda to each row of the data along axis 1 (the columns)\n",
    "trainData['net_capital'] = trainData.apply(lambda row: row.capital_gain - row.capital_loss, axis = 1) \n",
    "trainData['weightedBuyingPower'] = trainData.apply(lambda row: row.education_num * row.hoursPerWeek, axis = 1)\n",
    "#take the log of age and hoursPerWeek\n",
    "trainData['ageLog'] = trainData.apply(lambda row: np.log(row.age), axis = 1)\n",
    "trainData['hoursPerWeekLog'] = trainData.apply(lambda row: np.log(row.hoursPerWeek), axis = 1)\n",
    "trainData.drop(columns='age', inplace = True)\n",
    "trainData.drop(columns='hoursPerWeek', inplace = True)\n",
    "#Create crossed columns:\n",
    "trainData['GenderOccupation'] = trainData.apply(lambda row: row.occupation + row.sex, axis = 1)\n",
    "trainData['EducationGender'] = trainData.apply(lambda row: row.sex + row.education, axis = 1)\n",
    "trainData['EducationRace'] = trainData.apply(lambda row: row.education + row.race, axis = 1)\n",
    "\n",
    "#drop redundant column\n",
    "trainData.drop(columns='education', inplace = True)\n",
    "\n",
    "\n",
    "#TEST DATA LOADING, FEATURE CROSSES, NEW FEATURE CREATION, AND TRANSFORMS\n",
    "#load test data and do the same things to it that we did to the train set\n",
    "test = pd.read_csv(\"./Pr4Data/test-features.csv\", header=None,\n",
    "                       names = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\", \n",
    "                                \"marital_status\", \"occupation\", \"relationship\",\n",
    "                                \"race\", \"sex\", \"capital_gain\", \"capital_loss\", \"hoursPerWeek\", \"nativeCountry\"])\n",
    "#Everything done same as trainData\n",
    "test['net_capital'] = test.apply(lambda row: row.capital_gain - row.capital_loss, axis = 1) \n",
    "test['weightedBuyingPower'] = test.apply(lambda row: row.education_num * row.hoursPerWeek, axis = 1) \n",
    "test['ageLog'] = test.apply(lambda row: np.log(row.age), axis = 1)\n",
    "test['hoursPerWeekLog'] = test.apply(lambda row: np.log(row.hoursPerWeek), axis = 1)\n",
    "test.drop(columns='age', inplace = True)\n",
    "test.drop(columns='hoursPerWeek', inplace = True)\n",
    "test['GenderOccupation'] = test.apply(lambda row: row.occupation + row.sex, axis = 1)\n",
    "test['EducationGender'] = test.apply(lambda row: row.sex + row.education, axis = 1)\n",
    "test['EducationRace'] = test.apply(lambda row: row.education + row.race, axis = 1)\n",
    "test.drop(columns='education', inplace = True)\n",
    "\n",
    "#aggregate the train and test data so that we can train our label encoder and our normalization properly\n",
    "dataToTrainLabelEncoders = pd.concat([trainData,test], ignore_index=True)\n",
    "\n",
    "#use label encoder on the categorical columns\n",
    "listOfLabelEncoders = []\n",
    "listOfColumns = list(dataToTrainLabelEncoders.columns)\n",
    "l=0\n",
    "#for each column we will train and fit our label encoder\n",
    "for i in range(0, len(listOfColumns)):\n",
    "    if isinstance(dataToTrainLabelEncoders[listOfColumns[i]][0], str):\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        #Note: don't need to remove non categorical columns from consideration, because this \n",
    "        #method won't affect quant cols\n",
    "        le.fit(dataToTrainLabelEncoders[listOfColumns[i]])\n",
    "        listOfLabelEncoders.append(le)\n",
    "        dataToTrainLabelEncoders[listOfColumns[i]] = listOfLabelEncoders[l].transform(\n",
    "                                                        dataToTrainLabelEncoders[listOfColumns[i]])\n",
    "        l+=1\n",
    "        \n",
    "#Use the label encoders trained above on the train data set so we can train our classifiers        \n",
    "listOfColumns = list(trainData.columns)\n",
    "l=0\n",
    "for i in range(0, len(listOfColumns)):\n",
    "    if isinstance(trainData[listOfColumns[i]][0], str):\n",
    "        trainData[listOfColumns[i]] = listOfLabelEncoders[l].transform(trainData[listOfColumns[i]])\n",
    "        l+=1\n",
    "\n",
    "#record the column with sex to use in finding my bagging indices\n",
    "trainingColumns = list(trainData.columns)\n",
    "genderIndex = trainingColumns.index('sex')\n",
    "#get trainResponse\n",
    "trainResponse = pd.read_csv(\"./Pr4Data/train-output.csv\", header=None)\n",
    "trainResponse = trainResponse.values #make it into a np array\n",
    "\n",
    "#Use a yeo-johnson transformer (the default of powerTransformer) to normalize and scale the data\n",
    "#NOTE: Unless you tell power transformer not to it will also scale the data (as well as normalize)\n",
    "scaler = preprocessing.PowerTransformer()\n",
    "scaler.fit(dataToTrainLabelEncoders)\n",
    "trainData = scaler.transform(trainData)\n",
    "\n",
    "#Split the data into test and train, then split again so we can know when our model is done training\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(trainData, trainResponse, test_size=0.1,random_state=11)\n",
    "train_x2, valid_x2, train_y2, valid_y2 = train_test_split(train_x, train_y, test_size=0.1,random_state = 15)\n",
    "\n",
    "#create 150 models\n",
    "N = 150 \n",
    "#each model will account for 4000 data points in each class\n",
    "numPerSample = 4000 \n",
    "#get the indices that we will create our models with\n",
    "bagging = baggingIndices(N,numPerSample, train_x2, train_y2, genderIndex)\n",
    "#initialize lists where we will keep all the models generated\n",
    "baggedModelsXG = []\n",
    "ConfidModelsXG = []\n",
    "baggedModelsCat = []\n",
    "ConfidModelsCat = []\n",
    "#params for XGBoost, fitted these basically through trail and error for the best results, though I understand that\n",
    "#There are a few heuristics to use.\n",
    "\n",
    "#Brief description of the paramters:\n",
    "#eta can be thought of as the step size, .3 seems to be the best choice for my model\n",
    "#max_depth is how many splits our tree can be, I set this to 6 (which is fairly deep to me) because I will use the\n",
    "#XGBoosted trees as slightly overfit models, and the catboosted trees as slightly underfit models\n",
    "#objective determines what XGBoost is trying to minimize, for binary classification use logistic\n",
    "#num parrallel tree determines how many boosted trees we will include set to 15 because this gave good results\n",
    "#lambda is a regularization parameter (think ridge regession)\n",
    "#tree_method: algorithm to construct the trees, used exact because I wanted more accuracy (took a hit on \n",
    "#preformance though)\n",
    "#max delta step: added this constraint to help with overfitting\n",
    "#NOTE: per the XGBoost documentation a dictionary is used for parameters, which is why I used a dictionary\n",
    "params = {\n",
    "        'eta': 0.3, \n",
    "        'max_depth': 6,  \n",
    "        'objective': 'binary:logistic',  \n",
    "        'num_parallel_tree' : 15,\n",
    "        'lambda': .1,\n",
    "        'tree_method': 'exact',\n",
    "        'max_delta_step' : 5}\n",
    "NumberOfSteps = 60  # The number of training iterations\n",
    "\n",
    "#for each set of 4 rows in my matrix of sampled rows (this is required because of the way I constructed the \n",
    "#bagging matrix)\n",
    "for i in range(0, int(bagging.shape[0]/4)):\n",
    "\n",
    "    #pick indicies that make the sexs represented in the data even\n",
    "    #This j will give the set of female indicies with exactly 4000 entries\n",
    "    j = i*4\n",
    "    trainIndicesForModel = np.zeros(train_x2.shape[0], dtype = bool)\n",
    "    trainIndicesForModel[bagging[j,:]] = True\n",
    "    #Choose the other row with 4000 indicies (the one for male indicies)\n",
    "    j+=1\n",
    "    trainIndicesForModel[bagging[j,:]] = True\n",
    "    dataToTrainIthModel = train_x2[trainIndicesForModel, :]\n",
    "    #Subset our response\n",
    "    responceToTrainIthModel = train_y2[trainIndicesForModel]\n",
    "    \n",
    "    #make dmatrices with these so we can train XGBoost\n",
    "    Dmatrix_train = xgb.DMatrix(dataToTrainIthModel, label=responceToTrainIthModel)\n",
    "    Dmatrix_test = xgb.DMatrix(valid_x2, label=None)\n",
    "    \n",
    "    \n",
    "    #train the ith model\n",
    "    clfXGB = xgb.train(params, Dmatrix_train, NumberOfSteps)\n",
    "    #append the trained model to our list of models\n",
    "    baggedModelsXG.append(clfXGB)\n",
    "\n",
    "    #the next set of code will get our confidence in the ith xgboost model\n",
    "    predictions = clfXGB.predict(Dmatrix_test)\n",
    "    #Get the best prediction in each line\n",
    "    mostConfidPred = np.zeros(predictions.shape[0])\n",
    "    for j in range(0, predictions.shape[0]):\n",
    "        if predictions[j] > .5:\n",
    "            mostConfidPred[j] = 1\n",
    "        else:\n",
    "            mostConfidPred[j] = 0\n",
    "    #Save this as our confidence in XGBoost:\n",
    "    XGConfid = accuracy_score(valid_y2, mostConfidPred)\n",
    "    #append our confidence in the ith model\n",
    "    ConfidModelsXG.append(XGConfid)\n",
    "    \n",
    "    #Cat boosting:\n",
    "    #learning rate can be thought of as step size\n",
    "    #silent keeps it from printing a ton of information\n",
    "    #Set max depth to 3 because I wanted this model to be slightly underfit\n",
    "    #num_trees is kind of like the number of iterations of training, set this to 500 because it should converge\n",
    "    #by then\n",
    "    #reg_lambda: also a regularization\n",
    "    #scale pos weight: this will try to account for the class imbalance in the response variable \n",
    "    clfCat = CatBoostClassifier(learning_rate = .6, silent = True, max_depth = 3, num_trees = 500, \n",
    "                                reg_lambda = .1, scale_pos_weight = 4/3)\n",
    "    clfCat.fit(dataToTrainIthModel, responceToTrainIthModel)\n",
    "    #Append the model to the list of cat boosted models\n",
    "    baggedModelsCat.append(clfCat)\n",
    "    #get the predictions\n",
    "    predictions = clfCat.predict(valid_x2)\n",
    "    #use the accuracy to find our confidence in the cat boosted models\n",
    "    CatConfid = accuracy_score(valid_y2, predictions)\n",
    "    ConfidModelsCat.append(CatConfid)\n",
    "\n",
    "\n",
    "predictions = baggedPredict(baggedModelsXG, ConfidModelsXG, baggedModelsCat, ConfidModelsCat, valid_x)\n",
    "#print our accuracy on the validation set\n",
    "print(\"Accuracy on the test set:\")\n",
    "print(accuracy_score(valid_y, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is my code for getting the submission documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open the submission file with writing capabilities\n",
    "output = open(\"Submission.csv\", 'w')\n",
    "#Overwrite anything that was previously written in the document (start the cursor at position 0)\n",
    "output.truncate(0)\n",
    "#Formot the first row\n",
    "output.write(\"Id,Category\" + \"\\n\")\n",
    "\n",
    "#Do the same preprocessing on the test set that we did on the train\n",
    "#Label encode\n",
    "listOfColumns = list(test.columns)\n",
    "l = 0\n",
    "for i in range(0, len(listOfColumns)):\n",
    "    if isinstance(test[listOfColumns[i]][0], str):\n",
    "        test[listOfColumns[i]] = listOfLabelEncoders[l].transform(test[listOfColumns[i]])\n",
    "        l+=1\n",
    "#Normalize with the same scaler we used to normalize the train data\n",
    "test = test.values\n",
    "test = scaler.transform(test)\n",
    "#Get our predictions\n",
    "predictions = baggedPredict(baggedModelsXG, ConfidModelsXG, baggedModelsCat, ConfidModelsCat, test)\n",
    "#Write all of the predictions in the format that Kaggle expects\n",
    "for i in range(0, test.shape[0]):\n",
    "    if predictions[i] <.5:\n",
    "        output.write(str(i) + ',' + str(0) + \"\\n\")\n",
    "    else:\n",
    "        output.write(str(i) + ',' + str(1) + \"\\n\")\n",
    "#Close the file\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full citations:    \n",
    "SciKitLearn:  \n",
    "Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., … Duchesnay, É. (1970, January 1). Scikit-learn: Machine Learning in Python. Retrieved from http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html  \n",
    "Pandas:  \n",
    "Mckinney, W. (2010). Data Structures for Statistical Computing in Python. Proceedings of the 9th Python in Science Conference. doi: 10.25080/majora-92bf1922-00a  \n",
    "XGBoost:  \n",
    "Chen, T., & Guestrin, C. (2016). XGBoost. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD 16. doi: 10.1145/2939672.2939785  \n",
    "CatBoost:  \n",
    "Prokhorenkova, Liudmila, Gusev, Gleb, Vorobev, Aleksandr, … Andrey. (2019, January 20). CatBoost: unbiased boosting with categorical features. Retrieved from https://arxiv.org/abs/1706.09516  \n",
    "numpy:  \n",
    "Walt, S. V. D., Colbert, S. C., & Varoquaux, G. (2011). The NumPy Array: A Structure for Efficient Numerical Computation. Computing in Science & Engineering, 13(2), 22–30. doi: 10.1109/mcse.2011.37 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
